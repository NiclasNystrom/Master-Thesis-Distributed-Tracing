1 chart(s) linted, no failures

2019-03-13T05:59:16+01:00 INF prepare done.
2019-03-13T05:59:16+01:00 INF to have the desired state applied, run apply.
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ clear

niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ netroundsctl apply
2019-03-13T05:59:42+01:00 INF cluster: apply
2019-03-13T05:59:42+01:00 INF cluster: spec path: /home/niclas/work/tmp1/tmp/tmp/ncc3/env/eks/ncc.yaml
2019-03-13T05:59:42+01:00 INF cluster: state directory: /home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state
2019-03-13T05:59:42+01:00 INF aws-eks: apply ...
2019-03-13T05:59:42+01:00 INF running: terraform apply --auto-approve --input=false --var cluster_name="netrounds-niclas-19-eks" --var node_allowed_source_ips=["195.22.87.57/32"] --var ssh_public_key_path="~/.ssh/id_rsa.pub" --var vpc_cidr="10.35.0.0/16" /home/niclas/work/tmp1/tmp/tmp/ncc3/kubernetes/infrastructure/eks/eks-terraform
data.aws_region.current: Refreshing state...
data.aws_ami.eks_node_ami: Refreshing state...
data.aws_caller_identity.me: Refreshing state...
data.aws_availability_zones.available: Refreshing state...
aws_iam_policy.allow_route53_updates: Creating...
  arn:         "" => "<computed>"
  description: "" => "Allows Route53 updates"
  name:        "" => "netrounds-niclas-19-eks-allow-route53-updates"
  path:        "" => "/"
  policy:      "" => "{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n   {\n     \"Effect\": \"Allow\",\n     \"Action\": [ \"route53:ChangeResourceRecordSets\" ],\n     \"Resource\": [\"arn:aws:route53:::hostedzone/*\"]\n   },\n   {\n     \"Effect\": \"Allow\",\n     \"Action\": [\n       \"route53:ListHostedZones\",\n       \"route53:ListResourceRecordSets\"\n     ],\n     \"Resource\": [\"*\"]\n   }\n ]\n}\n"
aws_key_pair.ssh: Creating...
  fingerprint: "" => "<computed>"
  key_name:    "" => "netrounds-niclas-19-eks-node-ssh-key"
  public_key:  "" => "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7INoWXXhveZq7D23kfNiVFfbVTvnMe5cbF1jElOwzrv49C3cozHKkAYearojU8rTFYB01opZ/12750YJZNIs4bBCDS5ygLyLRftURF1BOs8tdHF9oF3bFQAN05dxcWA4CSPQPeanstZFBpGybV3cx+eQqXJu1x8J6l2cuBLL/YO35InXCVimdbQBSnC5DCfvioJkln+Qzkw/I0swIeKR/vMkpnlcw1Z4nMTqalqplaS0IDLqNRDeOpAvckwydCbwrIkx/injdHH5lhL8rxCWN0KUHbhAZAYwH29hDTM67Vl/aSL67UsRkdYmm/Mg4d9ZlYGsBXB7rnvqDAqGXb8YX niclas@niclas-vm"
aws_iam_role.eks_service_role: Creating...
  arn:                   "" => "<computed>"
  assume_role_policy:    "" => "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"eks.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n"
  create_date:           "" => "<computed>"
  force_detach_policies: "" => "false"
  max_session_duration:  "" => "3600"
  name:                  "" => "netrounds-niclas-19-eks-service-role"
  path:                  "" => "/"
  unique_id:             "" => "<computed>"
aws_vpc.eks_cluster_vpc: Creating...
  arn:                                                "" => "<computed>"
  assign_generated_ipv6_cidr_block:                   "" => "false"
  cidr_block:                                         "" => "10.35.0.0/16"
  default_network_acl_id:                             "" => "<computed>"
  default_route_table_id:                             "" => "<computed>"
  default_security_group_id:                          "" => "<computed>"
  dhcp_options_id:                                    "" => "<computed>"
  enable_classiclink:                                 "" => "<computed>"
  enable_classiclink_dns_support:                     "" => "<computed>"
  enable_dns_hostnames:                               "" => "<computed>"
  enable_dns_support:                                 "" => "true"
  instance_tenancy:                                   "" => "default"
  ipv6_association_id:                                "" => "<computed>"
  ipv6_cidr_block:                                    "" => "<computed>"
  main_route_table_id:                                "" => "<computed>"
  owner_id:                                           "" => "<computed>"
  tags.%:                                             "" => "2"
  tags.Name:                                          "" => "netrounds-niclas-19-eks-vpc"
  tags.kubernetes.io/cluster/netrounds-niclas-19-eks: "" => "shared"
aws_iam_role.worker_node_role: Creating...
  arn:                   "" => "<computed>"
  assume_role_policy:    "" => "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"ec2.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n"
  create_date:           "" => "<computed>"
  force_detach_policies: "" => "false"
  max_session_duration:  "" => "3600"
  name:                  "" => "netrounds-niclas-19-eks-node-role"
  path:                  "" => "/"
  unique_id:             "" => "<computed>"
aws_key_pair.ssh: Creation complete after 0s (ID: netrounds-niclas-19-eks-node-ssh-key)
aws_iam_role.eks_service_role: Creation complete after 2s (ID: netrounds-niclas-19-eks-service-role)
aws_iam_role.worker_node_role: Creation complete after 2s (ID: netrounds-niclas-19-eks-node-role)
aws_iam_role_policy_attachment.eks_service_policy: Creating...
  policy_arn: "" => "arn:aws:iam::aws:policy/AmazonEKSServicePolicy"
  role:       "" => "netrounds-niclas-19-eks-service-role"
aws_iam_role_policy_attachment.worker_node_policy: Creating...
  policy_arn: "" => "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
  role:       "" => "netrounds-niclas-19-eks-node-role"
aws_iam_role_policy_attachment.eks_cluster_policy: Creating...
  policy_arn: "" => "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
  role:       "" => "netrounds-niclas-19-eks-service-role"
data.template_file.aws_auth: Refreshing state...
aws_iam_instance_profile.worker_node: Creating...
  arn:         "" => "<computed>"
  create_date: "" => "<computed>"
  name:        "" => "netrounds-niclas-19-eks-worker-instance-profile"
  path:        "" => "/"
  role:        "" => "netrounds-niclas-19-eks-node-role"
  roles.#:     "" => "<computed>"
  unique_id:   "" => "<computed>"
aws_iam_role_policy_attachment.worker_node_cni_policy: Creating...
  policy_arn: "" => "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
  role:       "" => "netrounds-niclas-19-eks-node-role"
aws_iam_role_policy_attachment.worker_node_ecr_policy: Creating...
  policy_arn: "" => "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
  role:       "" => "netrounds-niclas-19-eks-node-role"
aws_iam_policy.allow_route53_updates: Creation complete after 2s (ID: arn:aws:iam::467560830515:policy/netrounds-niclas-19-eks-allow-route53-updates)
aws_iam_role_policy_attachment.allow_node_route53_update: Creating...
  policy_arn: "" => "arn:aws:iam::467560830515:policy/netrounds-niclas-19-eks-allow-route53-updates"
  role:       "" => "netrounds-niclas-19-eks-node-role"
aws_vpc.eks_cluster_vpc: Creation complete after 2s (ID: vpc-0f494e1f77a829943)
aws_subnet.eks_cluster_subnets[2]: Creating...
  arn:                                                "" => "<computed>"
  assign_ipv6_address_on_creation:                    "" => "false"
  availability_zone:                                  "" => "eu-north-1c"
  availability_zone_id:                               "" => "<computed>"
  cidr_block:                                         "" => "10.35.2.0/24"
  ipv6_cidr_block:                                    "" => "<computed>"
  ipv6_cidr_block_association_id:                     "" => "<computed>"
  map_public_ip_on_launch:                            "" => "false"
  owner_id:                                           "" => "<computed>"
  tags.%:                                             "" => "2"
  tags.Name:                                          "" => "netrounds-niclas-19-eks-subnet-2"
  tags.kubernetes.io/cluster/netrounds-niclas-19-eks: "" => "shared"
  vpc_id:                                             "" => "vpc-0f494e1f77a829943"
aws_subnet.eks_cluster_subnets[0]: Creating...
  arn:                                                "" => "<computed>"
  assign_ipv6_address_on_creation:                    "" => "false"
  availability_zone:                                  "" => "eu-north-1a"
  availability_zone_id:                               "" => "<computed>"
  cidr_block:                                         "" => "10.35.0.0/24"
  ipv6_cidr_block:                                    "" => "<computed>"
  ipv6_cidr_block_association_id:                     "" => "<computed>"
  map_public_ip_on_launch:                            "" => "false"
  owner_id:                                           "" => "<computed>"
  tags.%:                                             "" => "2"
  tags.Name:                                          "" => "netrounds-niclas-19-eks-subnet-0"
  tags.kubernetes.io/cluster/netrounds-niclas-19-eks: "" => "shared"
  vpc_id:                                             "" => "vpc-0f494e1f77a829943"
aws_security_group.control_plane_sg: Creating...
  arn:                                                "" => "<computed>"
  description:                                        "" => "Security group for the cluster control plane communication with worker nodes."
  egress.#:                                           "" => "<computed>"
  ingress.#:                                          "" => "<computed>"
  name:                                               "" => "netrounds-niclas-19-eks-control-plane-sg"
  owner_id:                                           "" => "<computed>"
  revoke_rules_on_delete:                             "" => "false"
  tags.%:                                             "" => "2"
  tags.Name:                                          "" => "netrounds-niclas-19-eks-control-plane-sg"
  tags.kubernetes.io/cluster/netrounds-niclas-19-eks: "" => "owned"
  vpc_id:                                             "" => "vpc-0f494e1f77a829943"
aws_iam_role_policy_attachment.eks_service_policy: Creation complete after 1s (ID: netrounds-niclas-19-eks-service-role-20190313045951078300000001)
aws_subnet.eks_cluster_subnets[1]: Creating...
  arn:                                                "" => "<computed>"
  assign_ipv6_address_on_creation:                    "" => "false"
  availability_zone:                                  "" => "eu-north-1b"
  availability_zone_id:                               "" => "<computed>"
  cidr_block:                                         "" => "10.35.1.0/24"
  ipv6_cidr_block:                                    "" => "<computed>"
  ipv6_cidr_block_association_id:                     "" => "<computed>"
  map_public_ip_on_launch:                            "" => "false"
  owner_id:                                           "" => "<computed>"
  tags.%:                                             "" => "2"
  tags.Name:                                          "" => "netrounds-niclas-19-eks-subnet-1"
  tags.kubernetes.io/cluster/netrounds-niclas-19-eks: "" => "shared"
  vpc_id:                                             "" => "vpc-0f494e1f77a829943"
aws_iam_role_policy_attachment.worker_node_policy: Creation complete after 1s (ID: netrounds-niclas-19-eks-node-role-20190313045951086400000002)
aws_iam_role_policy_attachment.worker_node_cni_policy: Creation complete after 1s (ID: netrounds-niclas-19-eks-node-role-20190313045951097200000003)
aws_iam_role_policy_attachment.worker_node_ecr_policy: Creation complete after 1s (ID: netrounds-niclas-19-eks-node-role-20190313045951103400000005)
aws_internet_gateway.gw: Creating...
  owner_id:                                           "" => "<computed>"
  tags.%:                                             "0" => "2"
  tags.Name:                                          "" => "netrounds-niclas-19-eks-gw"
  tags.kubernetes.io/cluster/netrounds-niclas-19-eks: "" => "shared"
  vpc_id:                                             "" => "vpc-0f494e1f77a829943"
aws_iam_role_policy_attachment.eks_cluster_policy: Creation complete after 1s (ID: netrounds-niclas-19-eks-service-role-20190313045951099700000004)
aws_iam_instance_profile.worker_node: Creation complete after 1s (ID: netrounds-niclas-19-eks-worker-instance-profile)
aws_subnet.eks_cluster_subnets[0]: Creation complete after 1s (ID: subnet-01663064b2d97dcaf)
aws_security_group.worker_node_sg: Creating...
  arn:                                                "" => "<computed>"
  description:                                        "" => "Worker node security group."
  egress.#:                                           "" => "1"
  egress.482069346.cidr_blocks.#:                     "" => "1"
  egress.482069346.cidr_blocks.0:                     "" => "0.0.0.0/0"
  egress.482069346.description:                       "" => ""
  egress.482069346.from_port:                         "" => "0"
  egress.482069346.ipv6_cidr_blocks.#:                "" => "0"
  egress.482069346.prefix_list_ids.#:                 "" => "0"
  egress.482069346.protocol:                          "" => "-1"
  egress.482069346.security_groups.#:                 "" => "0"
  egress.482069346.self:                              "" => "false"
  egress.482069346.to_port:                           "" => "0"
  ingress.#:                                          "" => "<computed>"
  name:                                               "" => "netrounds-niclas-19-eks-worker-sg"
  owner_id:                                           "" => "<computed>"
  revoke_rules_on_delete:                             "" => "false"
  tags.%:                                             "" => "2"
  tags.Name:                                          "" => "netrounds-niclas-19-eks-node-sg"
  tags.kubernetes.io/cluster/netrounds-niclas-19-eks: "" => "owned"
  vpc_id:                                             "" => "vpc-0f494e1f77a829943"
aws_subnet.eks_cluster_subnets[2]: Creation complete after 1s (ID: subnet-0999ca49658c21409)
aws_security_group.control_plane_sg: Creation complete after 2s (ID: sg-0a22fb39cdd2179f8)
aws_security_group_rule.allow_all_outbound: Creating...
  cidr_blocks.#:            "" => "1"
  cidr_blocks.0:            "" => "0.0.0.0/0"
  from_port:                "" => "0"
  protocol:                 "" => "-1"
  security_group_id:        "" => "sg-0a22fb39cdd2179f8"
  self:                     "" => "false"
  source_security_group_id: "" => "<computed>"
  to_port:                  "" => "0"
  type:                     "" => "egress"
aws_iam_role_policy_attachment.allow_node_route53_update: Creation complete after 2s (ID: netrounds-niclas-19-eks-node-role-20190313045951663900000006)
aws_subnet.eks_cluster_subnets[1]: Creation complete after 1s (ID: subnet-062c5b7986251f735)
aws_eks_cluster.control_plane: Creating...
  arn:                                        "" => "<computed>"
  certificate_authority.#:                    "" => "<computed>"
  created_at:                                 "" => "<computed>"
  endpoint:                                   "" => "<computed>"
  name:                                       "" => "netrounds-niclas-19-eks"
  platform_version:                           "" => "<computed>"
  role_arn:                                   "" => "arn:aws:iam::467560830515:role/netrounds-niclas-19-eks-service-role"
  version:                                    "" => "1.11"
  vpc_config.#:                               "" => "1"
  vpc_config.0.security_group_ids.#:          "" => "1"
  vpc_config.0.security_group_ids.2582589267: "" => "sg-0a22fb39cdd2179f8"
  vpc_config.0.subnet_ids.#:                  "" => "3"
  vpc_config.0.subnet_ids.3292580562:         "" => "subnet-01663064b2d97dcaf"
  vpc_config.0.subnet_ids.3682676638:         "" => "subnet-0999ca49658c21409"
  vpc_config.0.subnet_ids.4256391994:         "" => "subnet-062c5b7986251f735"
  vpc_config.0.vpc_id:                        "" => "<computed>"
aws_internet_gateway.gw: Creation complete after 1s (ID: igw-07a01199e4ff3604e)
aws_route_table.rt: Creating...
  owner_id:                                           "" => "<computed>"
  propagating_vgws.#:                                 "" => "<computed>"
  route.#:                                            "" => "1"
  route.2154582036.cidr_block:                        "" => "0.0.0.0/0"
  route.2154582036.egress_only_gateway_id:            "" => ""
  route.2154582036.gateway_id:                        "" => "igw-07a01199e4ff3604e"
  route.2154582036.instance_id:                       "" => ""
  route.2154582036.ipv6_cidr_block:                   "" => ""
  route.2154582036.nat_gateway_id:                    "" => ""
  route.2154582036.network_interface_id:              "" => ""
  route.2154582036.transit_gateway_id:                "" => ""
  route.2154582036.vpc_peering_connection_id:         "" => ""
  tags.%:                                             "" => "2"
  tags.Name:                                          "" => "netrounds-niclas-19-eks-rt"
  tags.kubernetes.io/cluster/netrounds-niclas-19-eks: "" => "shared"
  vpc_id:                                             "" => "vpc-0f494e1f77a829943"
aws_security_group_rule.allow_all_outbound: Creation complete after 0s (ID: sgrule-3645018206)
aws_security_group.worker_node_sg: Creation complete after 2s (ID: sg-0d8b5ef93ce31249f)
aws_security_group_rule.eks_ingress_from_nodes: Creating...
  description:              "" => "Allow pods to communicate with the cluster API Server."
  from_port:                "" => "443"
  protocol:                 "" => "tcp"
  security_group_id:        "" => "sg-0a22fb39cdd2179f8"
  self:                     "" => "false"
  source_security_group_id: "" => "sg-0d8b5ef93ce31249f"
  to_port:                  "" => "443"
  type:                     "" => "ingress"
aws_security_group_rule.node_openings[1]: Creating...
  cidr_blocks.#:            "" => "1"
  cidr_blocks.0:            "" => "195.22.87.57/32"
  from_port:                "" => "30443"
  protocol:                 "" => "tcp"
  security_group_id:        "" => "sg-0d8b5ef93ce31249f"
  self:                     "" => "false"
  source_security_group_id: "" => "<computed>"
  to_port:                  "" => "30443"
  type:                     "" => "ingress"
aws_security_group_rule.worker_intracom: Creating...
  description:              "" => "Allow worker nodes to communicate with each other."
  from_port:                "" => "0"
  protocol:                 "" => "-1"
  security_group_id:        "" => "sg-0d8b5ef93ce31249f"
  self:                     "" => "false"
  source_security_group_id: "" => "sg-0d8b5ef93ce31249f"
  to_port:                  "" => "65535"
  type:                     "" => "ingress"
aws_security_group_rule.node_openings[0]: Creating...
  cidr_blocks.#:            "" => "1"
  cidr_blocks.0:            "" => "195.22.87.57/32"
  from_port:                "" => "22"
  protocol:                 "" => "tcp"
  security_group_id:        "" => "sg-0d8b5ef93ce31249f"
  self:                     "" => "false"
  source_security_group_id: "" => "<computed>"
  to_port:                  "" => "22"
  type:                     "" => "ingress"
aws_security_group_rule.worker_ingress_from_eks: Creating...
  description:              "" => "Allow worker Kubelets and pods to receive communication from the cluster control plane."
  from_port:                "" => "1025"
  protocol:                 "" => "tcp"
  security_group_id:        "" => "sg-0d8b5ef93ce31249f"
  self:                     "" => "false"
  source_security_group_id: "" => "sg-0a22fb39cdd2179f8"
  to_port:                  "" => "65535"
  type:                     "" => "ingress"
aws_route_table.rt: Creation complete after 1s (ID: rtb-0c28a76dedd56f6dc)
aws_route_table_association.rt_ass[2]: Creating...
  route_table_id: "" => "rtb-0c28a76dedd56f6dc"
  subnet_id:      "" => "subnet-0999ca49658c21409"
aws_route_table_association.rt_ass[0]: Creating...
  route_table_id: "" => "rtb-0c28a76dedd56f6dc"
  subnet_id:      "" => "subnet-01663064b2d97dcaf"
aws_route_table_association.rt_ass[1]: Creating...
  route_table_id: "" => "rtb-0c28a76dedd56f6dc"
  subnet_id:      "" => "subnet-062c5b7986251f735"
aws_route_table_association.rt_ass[0]: Creation complete after 0s (ID: rtbassoc-0ac4dba06c5a05f06)
aws_route_table_association.rt_ass[2]: Creation complete after 0s (ID: rtbassoc-0e62ea9dffd939703)
aws_route_table_association.rt_ass[1]: Creation complete after 0s (ID: rtbassoc-0cbafef1d4b3224b9)
aws_security_group_rule.eks_ingress_from_nodes: Creation complete after 0s (ID: sgrule-3059150068)
aws_security_group_rule.node_openings[0]: Creation complete after 1s (ID: sgrule-2226285215)
aws_security_group_rule.node_openings[1]: Creation complete after 1s (ID: sgrule-1877403811)
aws_security_group_rule.worker_intracom: Creation complete after 2s (ID: sgrule-756772564)
aws_security_group_rule.worker_ingress_from_eks: Creation complete after 2s (ID: sgrule-3035005999)
aws_eks_cluster.control_plane: Still creating... (10s elapsed)
aws_eks_cluster.control_plane: Still creating... (20s elapsed)
aws_eks_cluster.control_plane: Still creating... (30s elapsed)
aws_eks_cluster.control_plane: Still creating... (40s elapsed)
aws_eks_cluster.control_plane: Still creating... (50s elapsed)
aws_eks_cluster.control_plane: Still creating... (1m0s elapsed)
aws_eks_cluster.control_plane: Still creating... (1m10s elapsed)
aws_eks_cluster.control_plane: Still creating... (1m20s elapsed)
aws_eks_cluster.control_plane: Still creating... (1m30s elapsed)
aws_eks_cluster.control_plane: Still creating... (1m40s elapsed)
aws_eks_cluster.control_plane: Still creating... (1m50s elapsed)
aws_eks_cluster.control_plane: Still creating... (2m0s elapsed)
aws_eks_cluster.control_plane: Still creating... (2m10s elapsed)
aws_eks_cluster.control_plane: Still creating... (2m20s elapsed)
aws_eks_cluster.control_plane: Still creating... (2m30s elapsed)
aws_eks_cluster.control_plane: Still creating... (2m40s elapsed)
aws_eks_cluster.control_plane: Still creating... (2m50s elapsed)
aws_eks_cluster.control_plane: Still creating... (3m0s elapsed)
aws_eks_cluster.control_plane: Still creating... (3m10s elapsed)
aws_eks_cluster.control_plane: Still creating... (3m20s elapsed)
aws_eks_cluster.control_plane: Still creating... (3m30s elapsed)
aws_eks_cluster.control_plane: Still creating... (3m40s elapsed)
aws_eks_cluster.control_plane: Still creating... (3m50s elapsed)
aws_eks_cluster.control_plane: Still creating... (4m0s elapsed)
aws_eks_cluster.control_plane: Still creating... (4m10s elapsed)
aws_eks_cluster.control_plane: Still creating... (4m20s elapsed)
aws_eks_cluster.control_plane: Still creating... (4m30s elapsed)
aws_eks_cluster.control_plane: Still creating... (4m40s elapsed)
aws_eks_cluster.control_plane: Still creating... (4m50s elapsed)
aws_eks_cluster.control_plane: Still creating... (5m0s elapsed)
aws_eks_cluster.control_plane: Still creating... (5m10s elapsed)
aws_eks_cluster.control_plane: Still creating... (5m20s elapsed)
aws_eks_cluster.control_plane: Still creating... (5m30s elapsed)
aws_eks_cluster.control_plane: Still creating... (5m40s elapsed)
aws_eks_cluster.control_plane: Still creating... (5m50s elapsed)
aws_eks_cluster.control_plane: Still creating... (6m0s elapsed)
aws_eks_cluster.control_plane: Still creating... (6m10s elapsed)
aws_eks_cluster.control_plane: Still creating... (6m20s elapsed)
aws_eks_cluster.control_plane: Still creating... (6m30s elapsed)
aws_eks_cluster.control_plane: Still creating... (6m40s elapsed)
aws_eks_cluster.control_plane: Still creating... (6m50s elapsed)
aws_eks_cluster.control_plane: Still creating... (7m0s elapsed)
aws_eks_cluster.control_plane: Still creating... (7m10s elapsed)
aws_eks_cluster.control_plane: Still creating... (7m20s elapsed)
aws_eks_cluster.control_plane: Still creating... (7m30s elapsed)
aws_eks_cluster.control_plane: Still creating... (7m40s elapsed)
aws_eks_cluster.control_plane: Still creating... (7m50s elapsed)
aws_eks_cluster.control_plane: Still creating... (8m0s elapsed)
aws_eks_cluster.control_plane: Still creating... (8m10s elapsed)
aws_eks_cluster.control_plane: Still creating... (8m20s elapsed)
aws_eks_cluster.control_plane: Still creating... (8m30s elapsed)
aws_eks_cluster.control_plane: Creation complete after 8m34s (ID: netrounds-niclas-19-eks)
data.template_file.kubeconfig: Refreshing state...
aws_launch_configuration.worker_template: Creating...
  associate_public_ip_address:               "" => "true"
  ebs_block_device.#:                        "" => "<computed>"
  ebs_optimized:                             "" => "<computed>"
  enable_monitoring:                         "" => "true"
  iam_instance_profile:                      "" => "netrounds-niclas-19-eks-worker-instance-profile"
  image_id:                                  "" => "ami-0154b2479ba20f8bb"
  instance_type:                             "" => "m5.xlarge"
  key_name:                                  "" => "netrounds-niclas-19-eks-node-ssh-key"
  name:                                      "" => "<computed>"
  name_prefix:                               "" => "netrounds-niclas-19-eks-worker"
  root_block_device.#:                       "" => "1"
  root_block_device.0.delete_on_termination: "" => "true"
  root_block_device.0.iops:                  "" => "<computed>"
  root_block_device.0.volume_size:           "" => "20"
  root_block_device.0.volume_type:           "" => "gp2"
  security_groups.#:                         "" => "1"
  security_groups.2167603106:                "" => "sg-0d8b5ef93ce31249f"
  user_data_base64:                          "" => "IyEvYmluL2Jhc2gKc2V0IC1vIHh0cmFjZQoKIyBtYWtlIHN1cmUgdG8gaW5zdGFsbCBhbnkgbmV3IHNlY3VyaXR5IHBhdGNoZXMgZm9yIG91ciBpbWFnZQp5dW0gdXBkYXRlIC0tc2VjdXJpdHkKCi9ldGMvZWtzL2Jvb3RzdHJhcC5zaCAtLWFwaXNlcnZlci1lbmRwb2ludCAnaHR0cHM6Ly85RjIxNkVCNTkzMjMwODE1N0NDNjIxQ0IyREIxRkQxRC55bDQuZXUtbm9ydGgtMS5la3MuYW1hem9uYXdzLmNvbScgLS1iNjQtY2x1c3Rlci1jYSAnTFMwdExTMUNSVWRKVGlCRFJWSlVTVVpKUTBGVVJTMHRMUzB0Q2sxSlNVTjVSRU5EUVdKRFowRjNTVUpCWjBsQ1FVUkJUa0puYTNGb2EybEhPWGN3UWtGUmMwWkJSRUZXVFZKTmQwVlJXVVJXVVZGRVJYZHdjbVJYU213S1kyMDFiR1JIVm5wTlFqUllSRlJGTlUxRVRYaE5la0V4VFVSWmQwMUdiMWhFVkVrMVRVUk5lRTFFUVRGTlJGbDNUVVp2ZDBaVVJWUk5Ra1ZIUVRGVlJRcEJlRTFMWVROV2FWcFlTblZhV0ZKc1kzcERRMEZUU1hkRVVWbEtTMjlhU1doMlkwNUJVVVZDUWxGQlJHZG5SVkJCUkVORFFWRnZRMmRuUlVKQlRTdFBDbll2YkdJdlZUZENaVFZRVUVkTmFsTkpWM0UxTmtaS09ERkxiRzlpU0ZodWJYbEtSRUZPU2pSTmNuaHNTa3RJUlVSalZtSnRVbFZaYzNWUFlrMW5SRUVLUWxGNU1VeDBiR2hHUjFOVVpYcDZTakZuVmpaNVRXOTJSVmR6TUhsTFUwcDVObEUwVVhBellTczNTMDlLVnpOaU1GaDBNMDVqV0daVmFscGpRV0p5Y1FvMGMxTmxjMkZGTTNCUmVrbGhSMjVuTkZSRmFHNW9TbGRqV21GRk9FbGlMMkl5ZHpObFkzWlpZWE5GTm1ab1JYaDFaVzlWUzFoVlFqTkRUbE5WTjFWNUNreGhjVWRKVTNWNE1saExSa1JVTHk5a2FrVm1WazUyWnk5Qk9FNUpZMk5DWm5wbVdVRlFORTFGYzNsa1ZEUlJZVFp4Y1VJMWRXODNhVEU0T0hOR2FsTUtNMnM0YlM4cmMwNW1la0prWjBoaFdVUkpVMUZMZUZCc1ZtazNTVE53Ym1SbGVXMVlTM0JIUlM4ek5EaG9XVUl3WTFodGNXRmxSMm8yVDFCREsydGpRZ293U2xCblF5c3JhMUpEZVhWdFNWWTFiR293UTBGM1JVRkJZVTFxVFVORmQwUm5XVVJXVWpCUVFWRklMMEpCVVVSQlowdHJUVUU0UjBFeFZXUkZkMFZDQ2k5M1VVWk5RVTFDUVdZNGQwUlJXVXBMYjFwSmFIWmpUa0ZSUlV4Q1VVRkVaMmRGUWtGQ2QxcHZlbEJMWmtkMGMwMWhkRlpZVVZneWVXVk9jbmhHUkhvS0wzazBXRlpSVjNwNldIZHJjR2RyYVdkSEswdHlZbmhIZG5ONVpuUTRWblV4VmtwS2JVZDVNR3hwUnk5WlIzUkNRalpwWWxKS2QyOXlOM3B1UldRM1R3cDZabmRYYWsxWk5FRkRaazFsUTNwNGEwMW1ibWxsWjJWVksyWlBhMXBVVVRWeFNuQldRMnhIVW10MlpsQjBiVlZYU2xCbVF6TlFLemxUZVhaSk1IVlBDbVJpTmtkS2JDOXRiUzg0WVZndlFuZFdiMmxKVDAxcmJVVjFlRVpJZEhWMWVuVXlLMjlyTnpoRFNVNVRjbmRVZG0xdlF5OUljVll5YkRoclpVb3lTRThLU2tKaE5WWlNhbUpEWldSVVEyRlJjRWxQTDBaclltY3dVMDk2SzFObmRGaFhkazFEZHpGeE5sRkRSRGx1TkhCS2MxRlFkRVppYUVwek5GbHlOU3RpUXdwdVIwVlFOMVpxTTBwME0xcEdTek1yYUd4MU16RnphVmh0TVdsVmRXZzBaVEkyZFVSaWRXTmplbVZDVm1neVYxaEhOVUZ0U0ZndlQyTXhVVDBLTFMwdExTMUZUa1FnUTBWU1ZFbEdTVU5CVkVVdExTMHRMUW89JyAnbmV0cm91bmRzLW5pY2xhcy0xOS1la3MnCg=="
aws_launch_configuration.worker_template: Creation complete after 1s (ID: netrounds-niclas-19-eks-worker20190313050826942300000007)
aws_autoscaling_group.worker_group: Creating...
  arn:                                "" => "<computed>"
  default_cooldown:                   "" => "<computed>"
  desired_capacity:                   "" => "1"
  force_delete:                       "" => "false"
  health_check_grace_period:          "" => "300"
  health_check_type:                  "" => "<computed>"
  launch_configuration:               "" => "netrounds-niclas-19-eks-worker20190313050826942300000007"
  load_balancers.#:                   "" => "<computed>"
  max_size:                           "" => "2"
  metrics_granularity:                "" => "1Minute"
  min_size:                           "" => "1"
  name:                               "" => "netrounds-niclas-19-eks-workers"
  protect_from_scale_in:              "" => "false"
  service_linked_role_arn:            "" => "<computed>"
  tag.#:                              "" => "2"
  tag.3799903766.key:                 "" => "Name"
  tag.3799903766.propagate_at_launch: "" => "true"
  tag.3799903766.value:               "" => "netrounds-niclas-19-eks-worker"
  tag.622675180.key:                  "" => "kubernetes.io/cluster/netrounds-niclas-19-eks"
  tag.622675180.propagate_at_launch:  "" => "true"
  tag.622675180.value:                "" => "owned"
  target_group_arns.#:                "" => "<computed>"
  vpc_zone_identifier.#:              "" => "1"
  vpc_zone_identifier.787094244:      "" => "subnet-01663064b2d97dcaf"
  wait_for_capacity_timeout:          "" => "10m"
aws_autoscaling_group.worker_group: Still creating... (10s elapsed)
aws_autoscaling_group.worker_group: Still creating... (20s elapsed)
aws_autoscaling_group.worker_group: Still creating... (30s elapsed)
aws_autoscaling_group.worker_group: Creation complete after 37s (ID: netrounds-niclas-19-eks-workers)

Apply complete! Resources: 31 added, 0 changed, 0 destroyed.

Outputs:

apiserver_endpoint = https://9F216EB5932308157CC621CB2DB1FD1D.yl4.eu-north-1.eks.amazonaws.com
auth_configmap = apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
    - rolearn: arn:aws:iam::467560830515:role/netrounds-niclas-19-eks-node-role
      username: system:node:{{EC2PrivateDNSName}}
      groups:
        - system:bootstrappers
        - system:nodes

kubeconfig = apiVersion: v1
clusters:
- cluster:
    server: "https://9F216EB5932308157CC621CB2DB1FD1D.yl4.eu-north-1.eks.amazonaws.com"
    certificate-authority-data: "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNU1ETXhNekExTURZd01Gb1hEVEk1TURNeE1EQTFNRFl3TUZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTStPCnYvbGIvVTdCZTVQUEdNalNJV3E1NkZKODFLbG9iSFhubXlKREFOSjRNcnhsSktIRURjVmJtUlVZc3VPYk1nREEKQlF5MUx0bGhGR1NUZXp6SjFnVjZ5TW92RVdzMHlLU0p5NlE0UXAzYSs3S09KVzNiMFh0M05jWGZValpjQWJycQo0c1Nlc2FFM3BReklhR25nNFRFaG5oSldjWmFFOEliL2IydzNlY3ZZYXNFNmZoRXh1ZW9VS1hVQjNDTlNVN1V5CkxhcUdJU3V4MlhLRkRULy9kakVmVk52Zy9BOE5JY2NCZnpmWUFQNE1Fc3lkVDRRYTZxcUI1dW83aTE4OHNGalMKM2s4bS8rc05mekJkZ0hhWURJU1FLeFBsVmk3STNwbmRleW1YS3BHRS8zNDhoWUIwY1htcWFlR2o2T1BDK2tjQgowSlBnQysra1JDeXVtSVY1bGowQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFCd1pvelBLZkd0c01hdFZYUVgyeWVOcnhGRHoKL3k0WFZRV3p6WHdrcGdraWdHK0tyYnhHdnN5ZnQ4VnUxVkpKbUd5MGxpRy9ZR3RCQjZpYlJKd29yN3puRWQ3Twp6ZndXak1ZNEFDZk1lQ3p4a01mbmllZ2VVK2ZPa1pUUTVxSnBWQ2xHUmt2ZlB0bVVXSlBmQzNQKzlTeXZJMHVPCmRiNkdKbC9tbS84YVgvQndWb2lJT01rbUV1eEZIdHV1enUyK29rNzhDSU5TcndUdm1vQy9IcVYybDhrZUoySE8KSkJhNVZSamJDZWRUQ2FRcElPL0ZrYmcwU096K1NndFhXdk1DdzFxNlFDRDluNHBKc1FQdEZiaEpzNFlyNStiQwpuR0VQN1ZqM0p0M1pGSzMraGx1MzFzaVhtMWlVdWg0ZTI2dURidWNjemVCVmgyV1hHNUFtSFgvT2MxUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo="
  name: "netrounds-niclas-19-eks"
contexts:
- context:
    cluster: "netrounds-niclas-19-eks"
    user: aws
  name: "netrounds-niclas-19-eks"
current-context: "netrounds-niclas-19-eks"
kind: Config
preferences: {}
users:
- name: aws
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      command: aws-iam-authenticator
      args:
        - "token"
        - "-i"
        - "netrounds-niclas-19-eks"
      # - "-r"
      # - "arn:aws:iam::467560830515:user/niclas.nystrom@netrounds.com"
      # env:
        # - name: AWS_PROFILE
        #   value: "<aws-profile>"
2019-03-13T06:09:04+01:00 INF running: terraform output kubeconfig > /home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state/kubeconfig
2019-03-13T06:09:04+01:00 INF running: terraform output auth_configmap | kubectl apply -f -
configmap/aws-auth created
2019-03-13T06:09:07+01:00 INF software: apply
2019-03-13T06:09:07+01:00 INF software: spec path: /home/niclas/work/tmp1/tmp/tmp/ncc3/env/eks/ncc.yaml
2019-03-13T06:09:07+01:00 INF software: state directory: /home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state
2019-03-13T06:09:07+01:00 INF tiller: install: applying RBAC manifests ...
2019-03-13T06:09:07+01:00 INF running: kubectl apply -f /home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state/tiller-rbac.yaml
serviceaccount/tiller created
clusterrolebinding.rbac.authorization.k8s.io/tiller created
2019-03-13T06:09:07+01:00 INF tiller: install: helm init ...
2019-03-13T06:09:07+01:00 INF running: helm init --wait --tiller-tls-hostname=tiller --service-account=tiller --tiller-tls --tiller-tls-cert=/home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state/tiller.pem --tiller-tls-key=/home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state/tiller-key.pem --tiller-tls-verify --tls-ca-cert=/home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state/ca.pem
$HELM_HOME has been configured at /home/niclas/.helm.

Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.
Happy Helming!
2019-03-13T06:09:38+01:00 INF Istio: install: helm add repo ...
2019-03-13T06:09:38+01:00 INF running: helm repo add istio.io https://storage.googleapis.com/istio-prerelease/daily-build/master-latest-daily/charts
"istio.io" has been added to your repositories
2019-03-13T06:09:39+01:00 INF Istio: install: helm update ...
2019-03-13T06:09:39+01:00 INF running: helm init --upgrade
$HELM_HOME has been configured at /home/niclas/.helm.

Tiller (the Helm server-side component) has been upgraded to the current version.
Happy Helming!
2019-03-13T06:09:39+01:00 INF Istio: install: helm init ...
2019-03-13T06:09:39+01:00 INF running: helm install /home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state/istio --name istio --namespace istio-system
NAME:   istio
LAST DEPLOYED: Wed Mar 13 06:09:40 2019
NAMESPACE: istio-system
STATUS: DEPLOYED

RESOURCES:
==> v1/ConfigMap
NAME                             DATA  AGE
istio                            1     32s
istio-galley-configuration       1     32s
istio-security-custom-resources  2     32s
istio-sidecar-injector           1     32s
istio-statsd-prom-bridge         1     32s
prometheus                       1     32s

==> v1/Pod(related)
NAME                                     READY  STATUS   RESTARTS  AGE
istio-citadel-796c94878b-wwtmf           1/1    Running  0         31s
istio-egressgateway-864444d6ff-n6bs4     1/1    Running  0         32s
istio-galley-6c68c5dbcf-wn7wm            0/1    Running  0         32s
istio-ingressgateway-694576c7bb-kkxbl    1/1    Running  0         32s
istio-pilot-79f5f46dd5-8hq2z             1/2    Running  0         32s
istio-policy-5bd5578b94-zvkz8            2/2    Running  0         32s
istio-sidecar-injector-6d8f88c98f-48xrt  0/1    Running  0         31s
istio-telemetry-5598f86cd8-6r98f         2/2    Running  0         32s
prometheus-76db5fddd5-6bnbc              1/1    Running  0         32s

==> v1/Service
NAME                    TYPE          CLUSTER-IP      EXTERNAL-IP       PORT(S)                                                                                                                  AGE
istio-citadel           ClusterIP     172.20.143.217  <none>            8060/TCP,9093/TCP                                                                                                        32s
istio-egressgateway     ClusterIP     172.20.57.80    <none>            80/TCP,443/TCP                                                                                                           32s
istio-galley            ClusterIP     172.20.187.245  <none>            443/TCP,9093/TCP                                                                                                         32s
istio-ingressgateway    LoadBalancer  172.20.145.100  a3556c02f454e...  80:31380/TCP,443:31390/TCP,31400:31400/TCP,15011:31617/TCP,8060:32104/TCP,853:32455/TCP,15030:32577/TCP,15031:31763/TCP  32s
istio-pilot             ClusterIP     172.20.236.231  <none>            15010/TCP,15011/TCP,8080/TCP,9093/TCP                                                                                    32s
istio-policy            ClusterIP     172.20.180.144  <none>            9091/TCP,15004/TCP,9093/TCP                                                                                              32s
istio-sidecar-injector  ClusterIP     172.20.226.11   <none>            443/TCP                                                                                                                  32s
istio-telemetry         ClusterIP     172.20.119.225  <none>            9091/TCP,15004/TCP,9093/TCP,42422/TCP                                                                                    32s
prometheus              ClusterIP     172.20.169.88   <none>            9090/TCP                                                                                                                 32s

==> v1/ServiceAccount
NAME                                    SECRETS  AGE
istio-citadel-service-account           1        32s
istio-egressgateway-service-account     1        32s
istio-galley-service-account            1        32s
istio-ingressgateway-service-account    1        32s
istio-mixer-service-account             1        32s
istio-pilot-service-account             1        32s
istio-security-post-install-account     1        32s
istio-sidecar-injector-service-account  1        32s
prometheus                              1        32s

==> v1alpha2/attributemanifest
NAME        AGE
istioproxy  32s
kubernetes  32s

==> v1alpha2/kubernetes
NAME        AGE
attributes  32s

==> v1alpha2/kubernetesenv
NAME     AGE
handler  32s

==> v1alpha2/logentry
NAME          AGE
accesslog     32s
tcpaccesslog  32s

==> v1alpha2/metric
NAME             AGE
requestcount     32s
requestduration  32s
requestsize      32s
responsesize     32s
tcpbytereceived  32s
tcpbytesent      32s

==> v1alpha2/prometheus
NAME     AGE
handler  32s

==> v1alpha2/rule
NAME                    AGE
kubeattrgenrulerule     32s
promhttp                32s
promtcp                 32s
stdio                   31s
stdiotcp                32s
tcpkubeattrgenrulerule  32s

==> v1alpha2/stdio
NAME     AGE
handler  31s

==> v1alpha3/DestinationRule
NAME             AGE
istio-policy     32s
istio-telemetry  32s

==> v1alpha3/Gateway
NAME                             AGE
istio-autogenerated-k8s-ingress  32s

==> v1beta1/ClusterRole
NAME                                      AGE
istio-citadel-istio-system                32s
istio-egressgateway-istio-system          32s
istio-galley-istio-system                 32s
istio-ingressgateway-istio-system         32s
istio-mixer-istio-system                  32s
istio-pilot-istio-system                  32s
istio-security-post-install-istio-system  32s
istio-sidecar-injector-istio-system       32s
prometheus-istio-system                   32s

==> v1beta1/ClusterRoleBinding
NAME                                                    AGE
istio-citadel-istio-system                              32s
istio-egressgateway-istio-system                        32s
istio-galley-admin-role-binding-istio-system            32s
istio-ingressgateway-istio-system                       32s
istio-mixer-admin-role-binding-istio-system             32s
istio-pilot-istio-system                                32s
istio-security-post-install-role-binding-istio-system   32s
istio-sidecar-injector-admin-role-binding-istio-system  32s
prometheus-istio-system                                 32s

==> v1beta1/Deployment
NAME                    READY  UP-TO-DATE  AVAILABLE  AGE
istio-citadel           1/1    1           1          32s
istio-egressgateway     1/1    1           1          32s
istio-galley            0/1    1           0          32s
istio-ingressgateway    1/1    1           1          32s
istio-pilot             0/1    1           0          32s
istio-policy            1/1    1           1          32s
istio-sidecar-injector  0/1    1           0          32s
istio-telemetry         1/1    1           1          32s
prometheus              1/1    1           1          32s

==> v1beta1/MutatingWebhookConfiguration
NAME                    AGE
istio-sidecar-injector  32s

==> v2beta1/HorizontalPodAutoscaler
NAME                  REFERENCE                        TARGETS        MINPODS  MAXPODS  REPLICAS  AGE
istio-egressgateway   Deployment/istio-egressgateway   <unknown>/80%  1        5        1         32s
istio-ingressgateway  Deployment/istio-ingressgateway  <unknown>/80%  1        5        1         32s
istio-pilot           Deployment/istio-pilot           <unknown>/80%  1        5        1         32s
istio-policy          Deployment/istio-policy          <unknown>/80%  1        5        1         32s
istio-telemetry       Deployment/istio-telemetry       <unknown>/80%  1        5        1         32s


2019-03-13T06:10:13+01:00 INF Istio: Enable namespace...
2019-03-13T06:10:13+01:00 INF running: kubectl label namespace default istio-injection=enabled on namespace: default
namespace/default labeled
2019-03-13T06:10:14+01:00 INF Istio: Enabled default for injection.
2019-03-13T06:10:14+01:00 INF helmfile: install: performing lint ...
2019-03-13T06:10:14+01:00 INF running: helmfile --file /home/niclas/work/tmp1/tmp/tmp/ncc3/env/helmfile.yaml --environment eks lint
Adding repo stable https://kubernetes-charts.storage.googleapis.com
"stable" has been added to your repositories

Updating repo
Hang tight while we grab the latest from your chart repositories...
...Skip local chart repository
...Successfully got an update from the "istio.io" chart repository
...Successfully got an update from the "stable" chart repository
Update Complete. ⎈ Happy Helming!⎈ 

Fetching stable/external-dns
Fetching stable/kubernetes-dashboard
Fetching stable/nginx-ingress
Linting /tmp/890884594/kubernetes-dashboard/~1.0.1/stable/kubernetes-dashboard/kubernetes-dashboard
==> Linting /tmp/890884594/kubernetes-dashboard/~1.0.1/stable/kubernetes-dashboard/kubernetes-dashboard
Lint OK

1 chart(s) linted, no failures

Linting /tmp/890884594/external-dns/~1.3.2/stable/external-dns/external-dns
==> Linting /tmp/890884594/external-dns/~1.3.2/stable/external-dns/external-dns
[INFO] Chart.yaml: icon is recommended

1 chart(s) linted, no failures

Linting /tmp/890884594/nginx-ingress/~1.1.2/stable/nginx-ingress/nginx-ingress
==> Linting /tmp/890884594/nginx-ingress/~1.1.2/stable/nginx-ingress/nginx-ingress
Lint OK

1 chart(s) linted, no failures

Linting /home/niclas/work/tmp1/tmp/tmp/ncc3/kubernetes/manifests/ncc-objects
==> Linting /home/niclas/work/tmp1/tmp/tmp/ncc3/kubernetes/manifests/ncc-objects
[INFO] Chart.yaml: icon is recommended

1 chart(s) linted, no failures

Linting /home/niclas/work/tmp1/tmp/tmp/ncc3/kubernetes/manifests/ncc-full
2019/03/13 06:10:20 Warning: Building values map for chart 'restol'. Skipped value (map[]) for 'deployment', as it is not a table.
2019/03/13 06:10:20 Warning: Building values map for chart 'restol'. Skipped value (map[]) for 'deployment', as it is not a table.
2019/03/13 06:10:20 Warning: Building values map for chart 'restol'. Skipped value (map[]) for 'deployment', as it is not a table.
2019/03/13 06:10:20 Warning: Building values map for chart 'restol'. Skipped value (map[]) for 'deployment', as it is not a table.
==> Linting /home/niclas/work/tmp1/tmp/tmp/ncc3/kubernetes/manifests/ncc-full
[INFO] Chart.yaml: icon is recommended

1 chart(s) linted, no failures

2019-03-13T06:10:20+01:00 INF helmfile: install: performing apply ...
2019-03-13T06:10:20+01:00 INF running: helmfile --file /home/niclas/work/tmp1/tmp/tmp/ncc3/env/helmfile.yaml --environment eks apply --args '--tls --tls-ca-cert=/home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state/ca.pem --tls-cert=/home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state/helm.pem --tls-key=/home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state/helm-key.pem'
--skip-repo-update has been deprecated. Provide --skip-deps instead.
Adding repo stable https://kubernetes-charts.storage.googleapis.com
"stable" has been added to your repositories

Updating repo
Hang tight while we grab the latest from your chart repositories...
...Skip local chart repository
...Successfully got an update from the "istio.io" chart repository
...Successfully got an update from the "stable" chart repository
Update Complete. ⎈ Happy Helming!⎈ 

Building dependency /home/niclas/work/tmp1/tmp/tmp/ncc3/kubernetes/manifests/ncc-objects
No requirements found in /home/niclas/work/tmp1/tmp/tmp/ncc3/kubernetes/manifests/ncc-objects/charts.

Building dependency /home/niclas/work/tmp1/tmp/tmp/ncc3/kubernetes/manifests/ncc-full
Hang tight while we grab the latest from your chart repositories...
...Unable to get an update from the "local" chart repository (http://127.0.0.1:8879/charts):
	Get http://127.0.0.1:8879/charts/index.yaml: dial tcp 127.0.0.1:8879: connect: connection refused
...Successfully got an update from the "istio.io" chart repository
...Successfully got an update from the "stable" chart repository
Update Complete. ⎈Happy Helming!⎈
Saving 1 charts
Downloading mysql from repo https://kubernetes-charts.storage.googleapis.com/
Deleting outdated charts

Comparing kubernetes-dashboard stable/kubernetes-dashboard
Comparing external-dns stable/external-dns
Comparing ncc-objects /home/niclas/work/tmp1/tmp/tmp/ncc3/kubernetes/manifests/ncc-objects
Comparing nginx-ingress stable/nginx-ingress
Comparing ncc-full /home/niclas/work/tmp1/tmp/tmp/ncc3/kubernetes/manifests/ncc-full
********************

	Release was not present in Helm.  Diff will show entire contents as new.

********************
default, agent-tls-secret, Secret (v1) has been added:
- 
+ # Source: ncc-objects/templates/agent-tls-secret.yaml
+ apiVersion: v1
+ kind: Secret
+ type: kubernetes.io/tls
+ metadata:
+   name: agent-tls-secret
+   namespace: default
+ data:
+   tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURLRENDQWhDZ0F3SUJBZ0lKQU9IQlhlZ0prd1lETUEwR0NTcUdTSWIzRFFFQkN3VUFNQ2t4SnpBbEJnTlYKQkFNTUhtNWpZeTFsYTNNdWNHOWpMblJsYzNRdWJtVjBjbTkxYm1SekxtTnZiVEFlRncweE9UQXlNVEV4TXpNeQpNamxhRncweU1EQXlNVEV4TXpNeU1qbGFNQ2t4SnpBbEJnTlZCQU1NSG01all5MWxhM011Y0c5akxuUmxjM1F1CmJtVjBjbTkxYm1SekxtTnZiVENDQVNJd0RRWUpLb1pJaHZjTkFRRUJCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFMV3AKdUVJeEpKT0g4dlp0ci9Qd3czYUFXSUViL2lLNGJZT0c0ejZRMXplYW83bjIzcnpTOXZKcHhpRXZ5Q0MzNUIwUQpoR2NjZHVaaWhjVmZVdDVRWDdSQm1GWHdTZExoeHlsOXlPRUdCbVRNVnJwazRGNU9ORGFVUUZtdFdMbldHa0VqClZvdkNtelhXTDlhUmF5cG5SWThHQ0kwTHVhODNLbzFYSGZnWUdtMEI5MmVId0N6OVZodDBNSzE5L1ZiV2UvME0KWVJ0d1BuQmtpaW5vKzVPVm93SEs0S2IyZUJtNlhPUXBEc0toamozRU1DenBoN0ZvWXN1UTlIWWhQUFowQVY2MApQTjJYSHVlQVlmSmNoUDRtUnNvaW83dWdEZE9YL1ZtL05JUTArRjVZMERNOWtpR3VyRHJmU2JsU2k5cUIrYzQ3Clc3MW9GRXlvOVNIdjRabVpKWmNDQXdFQUFhTlRNRkV3SFFZRFZSME9CQllFRk4rV3Q5eEN4a0EzYlNaTm9kWm0KQkZjaW93RHFNQjhHQTFVZEl3UVlNQmFBRk4rV3Q5eEN4a0EzYlNaTm9kWm1CRmNpb3dEcU1BOEdBMVVkRXdFQgovd1FGTUFNQkFmOHdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBRlJWcHJQTDdscGU5ODE4RytuOHYrSnlWWW4zCnEzTjlQNWttUW1DWnd6N3pOVlExNGdZZUl0UnBCdWdSMkZCakZPSUNET2cyMjNucVh5dnBldEdxblNjKzFDQkQKLzRUUjFjaHB1VHJNemZHVERkck9QTGd4Uzk2OUZqZno0RGxNQkVqWVl3bHhSUS9QVnB3UkxaWVZ1eEtHdjI5cQpZRG1PS0ZabVhxbkhINTBKcllpZkNhQXBNcCtFUVRibUdwTDFrT2VJYys5VllabkZnOXVqSmRQMU1VS002OTNwCjFMQ0Q2dEVNc0J1enVlYmZmK3NFanNYaGMrTndIeE9FZ3IrWW1SaVBNNnJGVXZ1YTZ0amtjLy9NZi93WUoyemYKaGZYT3dKTjdGWENtZUVUaEpRcGpuZTUwcHJMaTNpMXlIZXBJRWtLQlc5Q005T1NLdXhkRSt3NmJvVFU9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
+   tls.key: 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
default, ncc-tls-secret, Secret (v1) has been added:
- 
+ # Source: ncc-objects/templates/ncc-tls-secret.yaml
+ apiVersion: v1
+ kind: Secret
+ type: kubernetes.io/tls
+ metadata:
+   name: ncc-tls-secret
+   namespace: default
+ data:
+   tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURLRENDQWhDZ0F3SUJBZ0lKQU9IQlhlZ0prd1lETUEwR0NTcUdTSWIzRFFFQkN3VUFNQ2t4SnpBbEJnTlYKQkFNTUhtNWpZeTFsYTNNdWNHOWpMblJsYzNRdWJtVjBjbTkxYm1SekxtTnZiVEFlRncweE9UQXlNVEV4TXpNeQpNamxhRncweU1EQXlNVEV4TXpNeU1qbGFNQ2t4SnpBbEJnTlZCQU1NSG01all5MWxhM011Y0c5akxuUmxjM1F1CmJtVjBjbTkxYm1SekxtTnZiVENDQVNJd0RRWUpLb1pJaHZjTkFRRUJCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFMV3AKdUVJeEpKT0g4dlp0ci9Qd3czYUFXSUViL2lLNGJZT0c0ejZRMXplYW83bjIzcnpTOXZKcHhpRXZ5Q0MzNUIwUQpoR2NjZHVaaWhjVmZVdDVRWDdSQm1GWHdTZExoeHlsOXlPRUdCbVRNVnJwazRGNU9ORGFVUUZtdFdMbldHa0VqClZvdkNtelhXTDlhUmF5cG5SWThHQ0kwTHVhODNLbzFYSGZnWUdtMEI5MmVId0N6OVZodDBNSzE5L1ZiV2UvME0KWVJ0d1BuQmtpaW5vKzVPVm93SEs0S2IyZUJtNlhPUXBEc0toamozRU1DenBoN0ZvWXN1UTlIWWhQUFowQVY2MApQTjJYSHVlQVlmSmNoUDRtUnNvaW83dWdEZE9YL1ZtL05JUTArRjVZMERNOWtpR3VyRHJmU2JsU2k5cUIrYzQ3Clc3MW9GRXlvOVNIdjRabVpKWmNDQXdFQUFhTlRNRkV3SFFZRFZSME9CQllFRk4rV3Q5eEN4a0EzYlNaTm9kWm0KQkZjaW93RHFNQjhHQTFVZEl3UVlNQmFBRk4rV3Q5eEN4a0EzYlNaTm9kWm1CRmNpb3dEcU1BOEdBMVVkRXdFQgovd1FGTUFNQkFmOHdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBRlJWcHJQTDdscGU5ODE4RytuOHYrSnlWWW4zCnEzTjlQNWttUW1DWnd6N3pOVlExNGdZZUl0UnBCdWdSMkZCakZPSUNET2cyMjNucVh5dnBldEdxblNjKzFDQkQKLzRUUjFjaHB1VHJNemZHVERkck9QTGd4Uzk2OUZqZno0RGxNQkVqWVl3bHhSUS9QVnB3UkxaWVZ1eEtHdjI5cQpZRG1PS0ZabVhxbkhINTBKcllpZkNhQXBNcCtFUVRibUdwTDFrT2VJYys5VllabkZnOXVqSmRQMU1VS002OTNwCjFMQ0Q2dEVNc0J1enVlYmZmK3NFanNYaGMrTndIeE9FZ3IrWW1SaVBNNnJGVXZ1YTZ0amtjLy9NZi93WUoyemYKaGZYT3dKTjdGWENtZUVUaEpRcGpuZTUwcHJMaTNpMXlIZXBJRWtLQlc5Q005T1NLdXhkRSt3NmJvVFU9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
+   tls.key: LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk1JSUV2QUlCQURBTkJna3Foa2lHOXcwQkFRRUZBQVNDQktZd2dnU2lBZ0VBQW9JQkFRQzFxYmhDTVNTVGgvTDIKYmEvejhNTjJnRmlCRy80aXVHMkRodU0ra05jM21xTzU5dDY4MHZieWFjWWhMOGdndCtRZEVJUm5ISGJtWW9YRgpYMUxlVUYrMFFaaFY4RW5TNGNjcGZjamhCZ1prekZhNlpPQmVUalEybEVCWnJWaTUxaHBCSTFhTHdwczExaS9XCmtXc3FaMFdQQmdpTkM3bXZOeXFOVngzNEdCcHRBZmRuaDhBcy9WWWJkREN0ZmYxVzFudjlER0ViY0Q1d1pJb3AKNlB1VGxhTUJ5dUNtOW5nWnVsemtLUTdDb1k0OXhEQXM2WWV4YUdMTGtQUjJJVHoyZEFGZXREemRseDduZ0dIeQpYSVQrSmtiS0lxTzdvQTNUbC8xWnZ6U0VOUGhlV05BelBaSWhycXc2MzBtNVVvdmFnZm5PTzF1OWFCUk1xUFVoCjcrR1ptU1dYQWdNQkFBRUNnZ0VBSTZ2NUQ1NWd4VXVMelJJSzNBYnFDdW4yOXh0TkV2cE9IdWFzMXN0UVI0M3AKR29vOGdLMllhaytVNVUxaHFmTVNLeGJrT084ZzllcGd4RG9NcHJUM3BnaS9aaGl5Rm1QWlhPOWpNN1NnS3NqcApPdDYySkM2TkdDNlNXRW11dzRja1RxZ214WGpvUzNFTW9jQ2FYNUE1MjNUZTF2dDNjYzJPWUlTNzBNekMyNmFUCmdNcUo1b0xmaGpVcjdKb0xqRGQ4VG54Z09RVUloc3VKMEk5eDlqM3dyUElNRUsvTXMyZlAvSmtjQm9JSkdsRGMKamZsbGZlVndpS1RwWnN4R3Q5Z3JLTkNSa29MdFc0NXlkd1JTQnBENkJkMWtkcUU3N2RGVmpPTHQySSswYXNJNApILy9wYXRKVVZMYmNqWTBjcm5meEdlbTJRQ2JUek8wMXdmem9lenNDU1FLQmdRRFoxa29aQ0R6UklqNyttdExPCmQvb3J1eG5YMkYyYXR5UElIS3FqME8zS2lkY2liN1ZaMWpWVTh3d1UvaXVuYmFvSVkwSHRTV0JIRTZJdTlPRkkKL3BpYWNKbDJZcndGTHArMmxsTy83M1lWZnIveVJqRUZNbFJ6UEpNK3ZsSWY0MzdQcUxLR2laY1c3N29GUjM5cQpCOTZzTlJCNytFMHpKRGcrQkI0eHQrQkRCUUtCZ1FEVmZSR1l1R2FRU25TMTZDTjR2MEFWMEMvZkVrK0FJNDhCCkI3OTU3Q2RtVG1iU2pUR2RNZ1lSVFp6VmZabnZaa3NUQmQ3K3g0bkgyVkxrRU9IS1QzSjN1RXpSWEJKaWxHc0YKN0FyUlg0MWpiV250ZVltam5RcVVSbGNYeDZWSEoyMjFUV2wxUC9qaXR4NmpRK1lsSFF0WjBMdFA3WlhSV1I0ZwpkdklBNGQ4ZzZ3S0JnRWhRZHZDd1oyQVZ1a3ZUSWNBZzNBL0FZT2ZpajlCWWs2eE90K2NCNks2ZjY2Y201bEVZClowUDRHejZzMGRrVzhxY3VMQ2lWZnp5WksvSGlvUmNXVFpxWFhwcUtWRmgwRGRrVE5tSTFJRUlxZlpYejd6b0cKWnpxS0ZXZHl6dGgvTVEzR3R0MHF0OW1KWTN0MUxMc0tPY1Y1S0N1L1ZnRndPS3Q5dW5oT3prZnhBb0dBREY5QgpJODZiOHdTOW1zVW5PUm9zUnBkWUR0OXF5QXhIZjFISm9QRTBuMU12MWtma2RpMnQrRmE5SGdvM1g0NVlEM1FwCnBpNCtKOXpLVnZrN2ZKUDZHRUlRQVpvS1hyZ25NNktvUmRYNXhhZFRtaldPNm5KeGFJSmpEYmUrTms5c3BqTWsKUjIwN3FUZmZpS3FvcmNIdkpIZGFRNW1MTXlTdEFFaWtZYlMxVDUwQ2dZQURlZ2pvekZ0UkFBYzhpbzFCMUlabQp0U0ZaWkg4dHJkY2pXSk1VbHhiQU12ZFJtanM4M2RmdEY3WWY5Mkl6MTFKTVFFOHcrS245UUZBWk5IZkhON2FDCkN3SXlpcXFodG1tOXF0aW1Td0FoUURzVzVKY3pDVzVvUWpldm5Cejk4NWVkWHhoL25NcVgzbUs1MDh4TWRNUWUKRGpCNm85Zyt5U3NYYlhuODBURm5zdz09Ci0tLS0tRU5EIFBSSVZBVEUgS0VZLS0tLS0K
default, aws-mysql, StorageClass (storage.k8s.io) has been added:
- 
+ # Source: ncc-objects/templates/aws-mysql-storage-class.yaml
+ kind: StorageClass
+ apiVersion: storage.k8s.io/v1
+ metadata:
+   name: aws-mysql
+ provisioner: kubernetes.io/aws-ebs
+ parameters:
+   type: io1
+   iopsPerGB: "3"
+ reclaimPolicy: Retain
+ mountOptions:
+   - debug
default, aws-ncc, StorageClass (storage.k8s.io) has been added:
- 
+ # Source: ncc-objects/templates/aws-ncc-storage-class.yaml
+ kind: StorageClass
+ apiVersion: storage.k8s.io/v1
+ metadata:
+   name: aws-ncc
+ provisioner: kubernetes.io/aws-ebs
+ parameters:
+   type: io1
+   iopsPerGB: "3"
+ reclaimPolicy: Retain
+ mountOptions:
+   - debug
default, aws-gp2, StorageClass (storage.k8s.io) has been added:
- 
+ # Source: ncc-objects/templates/gp2-storage-class.yaml
+ kind: StorageClass
+ apiVersion: storage.k8s.io/v1
+ metadata:
+   # Since version 1.11 of AWS EKS, a 'gp2' storage class is already created
+   # (which uses reclaimPolicy: Delete). To avoid name clashes we name this
+   # storage class differently.
+   name: aws-gp2
+ provisioner: kubernetes.io/aws-ebs
+ parameters:
+   type: gp2
+ reclaimPolicy: Retain
+ mountOptions:
+   - debug
default, ncc-objects-mysql-storage, PersistentVolumeClaim (v1) has been added:
- 
+ # Source: ncc-objects/templates/pvc.yaml
+ kind: PersistentVolumeClaim
+ apiVersion: v1
+ metadata:
+   name: ncc-objects-mysql-storage
+   labels:
+     app: ncc-objects-mysql-storage
+     chart: "ncc-objects-0.1.2"
+     release: "ncc-objects"
+     heritage: "Tiller"
+ spec:
+   accessModes:
+     - "ReadWriteOnce"
+   resources:
+     requests:
+       storage: "100Gi"
+   storageClassName: "aws-gp2"
default, ncc-objects-ncc-storage, PersistentVolumeClaim (v1) has been added:
- 
+ # Source: ncc-objects/templates/pvc.yaml
+ kind: PersistentVolumeClaim
+ apiVersion: v1
+ metadata:
+   name: ncc-objects-ncc-storage
+   labels:
+     app: ncc-objects-ncc-storage
+     chart: "ncc-objects-0.1.2"
+     release: "ncc-objects"
+     heritage: "Tiller"
+ spec:
+   accessModes:
+     - "ReadWriteOnce"
+   resources:
+     requests:
+       storage: "100Gi"
+   storageClassName: "aws-gp2"
default, ncc-objects-limits, LimitRange (v1) has been added:
- 
+ # Source: ncc-objects/templates/limit-range.yaml
+ apiVersion: v1
+ kind: LimitRange
+ metadata:
+   name: ncc-objects-limits
+ spec:
+   limits:
+   - default:
+       {}
+       
+     defaultRequest:
+       cpu: 50m
+       memory: 100Mi
+       
+     type: Container
Error: identified at least one change, exiting with non-zero exit code (detailed-exitcode parameter enabled)
identified at least one change, exiting with non-zero exit code (detailed-exitcode parameter enabled)
Error: plugin "diff" exited with error

********************

	Release was not present in Helm.  Diff will show entire contents as new.

********************
2019/03/13 06:10:29 warning: skipped value for deployment: Not a table.
2019/03/13 06:10:29 warning: skipped value for deployment: Not a table.
2019/03/13 06:10:30 warning: skipped value for deployment: Not a table.
2019/03/13 06:10:30 warning: skipped value for deployment: Not a table.
default, agent-ws-server-conf-configmap, ConfigMap (v1) has been added:
- 
+ # Source: ncc-full/charts/agent-ws-server/templates/configmap.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   name: agent-ws-server-conf-configmap
+ data:
+   agent_ws_server.conf: |-
+     nossl = True
+     ws_listen_port = 9200
+     daemon_host = 'ncc.default.svc.cluster.local'
+     advertised_proxy_host = 'agent-ws-server.default.svc.cluster.local'
+     proxy_listen_interface = '0.0.0.0'
default, netrounds6-conf-configmap, ConfigMap (v1) has been added:
+ # Source: ncc-full/charts/ncc/templates/configmaps/openvpn.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   name: netrounds6-conf-configmap
+ data:
+   netrounds6.conf: |-
+     # IPv6 Openvpn config for Netrounds Test Agent management connection.
+     #
+     # NOTE: This requires Openvpn version >= 2.4.4
+     #
+ 
+     proto tcp6
+     port 6000
+ 
+     dev tun
+ 
+     ca /var/lib/netrounds/openvpn/ca.crt
+     cert /var/lib/netrounds/openvpn/server.crt
+     key /var/lib/netrounds/openvpn/server.key
+     dh /var/lib/netrounds/openvpn/dh.pem
+ 
+     server 100.70.0.0 255.255.248.0
+ 
+     topology subnet
+     keepalive 5 30
+     persist-key
+     persist-tun
+     verb 3
+     script-security 3
+ 
+     max-clients 2048
+ 
+     client-connect /usr/bin/client-connect
+     client-disconnect /usr/bin/client-disconnect
+ 
+     up /etc/openvpn/netrounds-iptables-setup.sh
+     plugin /usr/lib/x86_64-linux-gnu/openvpn/plugins/openvpn-plugin-down-root.so /etc/openvpn/netrounds-iptables-teardown.sh
+ 
+     # This is the host used for the initial register call done by the TA which requires HTTPS.
+     # Within the cluster we are not using TLS encryption between services. This is being handled by K8 Ingress.
+     # This is the reason why openvpn connects using external NCC domain instead of localhost
+     port-share app.netrounds-niclas-19-eks.test.netrounds.com 443
  
+     push "setenv-safe NETROUNDS_hosts genalyzerservice=100.70.0.1;update=100.70.0.1"
default, netrounds-restol-standalone-nossl-conf-configmap, ConfigMap (v1) has been added:
+ # Source: ncc-full/charts/restol/templates/configmap.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   name: netrounds-restol-standalone-nossl-conf-configmap
+ data:
+   netrounds-restol-standalone-nossl.conf: |-
+     <VirtualHost *:80>
+       ServerName netrounds_restol
+       ServerAdmin support@netrounds.com
+ 
+       DocumentRoot "/usr/lib/python2.7/dist-packages/restol/"
+ 
+       <Directory /usr/lib/python2.7/dist-packages/restol/>
+           AllowOverride None
+           Require all granted
+       </Directory>
+ 
+       # Logging
+       ErrorLogFormat "%M"
+       ErrorLog /dev/stdout
+       CustomLog /dev/stdout combined
  
+       WSGIProcessGroup netrounds_restol
+       WSGIDaemonProcess netrounds_restol user=www-data group=www-data processes=4 threads=1
+       WSGIScriptAlias /restol /usr/lib/python2.7/dist-packages/restol/netrounds_restol.wsgi
+       WSGIScriptAlias /rest /usr/lib/python2.7/dist-packages/restol/netrounds_restol.wsgi
+       WSGIPassAuthorization On
+     </VirtualHost>
default, mysql, Service (v1) has been added:
- 
+ # Source: ncc-full/charts/mysql/templates/svc.yaml
+ apiVersion: v1
+ kind: Service
+ metadata:
+   name: mysql
+   labels:
+     app: mysql
+     chart: "mysql-0.10.2"
+     release: "ncc-full"
+     heritage: "Tiller"
+ spec:
+   type: ClusterIP
+   ports:
+   - name: mysql
+     port: 3306
+     targetPort: mysql
+   selector:
+     app: mysql
default, agent-ws-server, Deployment (apps) has been added:
- 
+ # Source: ncc-full/charts/agent-ws-server/templates/deployment.yaml
+ apiVersion: apps/v1beta2
+ kind: Deployment
+ metadata:
+   name: agent-ws-server
+   labels:
+     app.kubernetes.io/name: agent-ws-server
+     helm.sh/chart: agent-ws-server-0.1.3
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+ spec:
+   replicas: 1
+   selector:
+     matchLabels:
+       app.kubernetes.io/name: agent-ws-server
+       app.kubernetes.io/instance: ncc-full
+   template:
+     metadata:
+       labels:
+         app.kubernetes.io/name: agent-ws-server
+         app.kubernetes.io/instance: ncc-full
+     spec:
+       volumes:
+         - name: agent-ws-server-conf-configmap
+           configMap:
+             name: agent-ws-server-conf-configmap
+       imagePullSecrets:
+       - name: awsecr-cred
+       
+       containers:
+         - name: agent-ws-server
+           image: "467560830515.dkr.ecr.eu-west-1.amazonaws.com/netrounds/agent-ws-server:2.28.0"
+           imagePullPolicy: IfNotPresent
+           resources:
+             limits: {}
+             requests:
+               cpu: 100m
+               memory: 128Mi
+             
+           readinessProbe:
+             tcpSocket:
+               port: 9200
+             initialDelaySeconds: 10
+             periodSeconds: 10
+           volumeMounts:
+             - name: agent-ws-server-conf-configmap
+               mountPath: /etc/netrounds/agent_ws_server.conf
+               subPath: agent_ws_server.conf
+           ports:
+             - name: agent-ws-server
+               containerPort: 9200
+               protocol: TCP
+             - name: zmq
+               containerPort: 9300
+               protocol: TCP
default, probe-connect-conf-secret, Secret (v1) has been added:
- 
+ # Source: ncc-full/charts/ncc/templates/secrets/openvpn.yaml
+ apiVersion: v1
+ kind: Secret
+ metadata:
+   name: probe-connect-conf-secret
+   labels:
+     app.kubernetes.io/name: ncc
+     helm.sh/chart: ncc-0.1.3
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+ type: Opaque
+ data:
+   probe-connect.conf: SE9TVE5BTUU9bXlzcWwuZGVmYXVsdC5zdmMuY2x1c3Rlci5sb2NhbApQT1JUPTMzMDYKREFUQUJBU0U9bmV0cm91bmRzClVTRVI9bmV0cm91bmRzClBBU1NXT1JEPW5ldHJvdW5kcwo=
default, netrounds-conf-configmap, ConfigMap (v1) has been added:
+ # Source: ncc-full/charts/ncc/templates/configmaps/openvpn.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   name: netrounds-conf-configmap
+ data:
+   netrounds.conf: |-
+     proto tcp
+     port 6000
+ 
+     dev tun
+ 
+     ca /var/lib/netrounds/openvpn/ca.crt
+     cert /var/lib/netrounds/openvpn/server.crt
+     key /var/lib/netrounds/openvpn/server.key
+     dh /var/lib/netrounds/openvpn/dh.pem
+ 
+     server 100.70.0.0 255.255.248.0
+ 
+     topology subnet
+     keepalive 5 30
+     persist-key
+     persist-tun
+     verb 3
+     script-security 3
+ 
+     max-clients 2048
+ 
+     client-connect /usr/bin/client-connect
+     client-disconnect /usr/bin/client-disconnect
+ 
+     up /etc/openvpn/netrounds-iptables-setup.sh
+     plugin /usr/lib/openvpn/openvpn-plugin-down-root.so /etc/openvpn/netrounds-iptables-teardown.sh
+ 
+     # This is the host used for the initial register call done by the TA which requires HTTPS.
+     # Within the cluster we are not using TLS encryption between services. This is being handled by K8 Ingress.
+     # This is the reason why openvpn connects using external NCC domain instead of localhost
+     port-share app.netrounds-niclas-19-eks.test.netrounds.com 443
  
+     push "setenv-safe NETROUNDS_hosts genalyzerservice=100.70.0.1;update=100.70.0.1"
default, agent-ws-server, Service (v1) has been added:
- 
+ # Source: ncc-full/charts/agent-ws-server/templates/service.yaml
+ apiVersion: v1
+ kind: Service
+ metadata:
+   name: agent-ws-server
+   labels:
+     app.kubernetes.io/name: agent-ws-server
+     helm.sh/chart: agent-ws-server-0.1.3
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+ spec:
+   type: ClusterIP
+   ports:
+     - port: 9200
+       targetPort: 9200
+       protocol: TCP
+       name: agent-ws-server
+     - port: 9300
+       targetPort: 9300
+       protocol: TCP
+       name: zmq
+   selector:
+     app.kubernetes.io/name: agent-ws-server
+     app.kubernetes.io/instance: ncc-full
default, restol, Deployment (apps) has been added:
- 
+ # Source: ncc-full/charts/restol/templates/deployment.yaml
+ apiVersion: apps/v1beta2
+ kind: Deployment
+ metadata:
+   name: restol
+   labels:
+     app.kubernetes.io/name: restol
+     helm.sh/chart: restol-0.1.3
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+ spec:
+   replicas: 1
+   selector:
+     matchLabels:
+       app.kubernetes.io/name: restol
+       app.kubernetes.io/instance: ncc-full
+   template:
+     metadata:
+       labels:
+         app.kubernetes.io/name: restol
+         app.kubernetes.io/instance: ncc-full
+     spec:
+       volumes:
+         - name: restol-conf-configmap
+           configMap:
+             name: restol-conf-configmap
+         - name: netrounds-restol-standalone-nossl-conf-configmap
+           configMap:
+             name: netrounds-restol-standalone-nossl-conf-configmap
+       imagePullSecrets:
+       - name: awsecr-cred
+       
+       containers:
+         - name: restol
+           image: "467560830515.dkr.ecr.eu-west-1.amazonaws.com/netrounds/rest-orchestration:2.28.0"
+           imagePullPolicy: IfNotPresent
+           resources:
+             limits: {}
+             requests:
+               cpu: 100m
+               memory: 128Mi
+             
+           livenessProbe:
+             httpGet:
+               path: /rest
+               port: 80
+             initialDelaySeconds: 10
+             periodSeconds: 10
+           readinessProbe:
+             httpGet:
+               path: /rest
+               port: 80
+             initialDelaySeconds: 10
+             periodSeconds: 10
+           ports:
+             - name: restol
+               containerPort: 80
+               protocol: TCP
+           volumeMounts:
+             - name: restol-conf-configmap
+               mountPath: /etc/netrounds/restol.conf
+               subPath: restol.conf
+             - name: netrounds-restol-standalone-nossl-conf-configmap
+               mountPath: /etc/apache2/sites-available/netrounds-restol-standalone-nossl.conf
+               subPath: netrounds-restol-standalone-nossl.conf
default, ncc, Ingress (extensions) has been added:
- 
+ # Source: ncc-full/charts/ncc/templates/ingress.yaml
+ apiVersion: extensions/v1beta1
+ kind: Ingress
+ metadata:
+   name: ncc
+   labels:
+     app.kubernetes.io/name: ncc
+     helm.sh/chart: ncc-0.1.3
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+   annotations:
+     kubernetes.io/ingress.class: nginx
+     
+ spec:
+   tls:
+     - hosts:
+         - "app.netrounds-niclas-19-eks.test.netrounds.com"
+         - "login.netrounds-niclas-19-eks.test.netrounds.com"
+       secretName: ncc-tls-secret
+   rules:
+     - host: "app.netrounds-niclas-19-eks.test.netrounds.com"
+       http:
+         paths:
+           - path: /
+             backend:
+               serviceName: ncc
+               servicePort: 80
+     # The NCC server needs to be reachable on the openvpnHost domain name also,
+     # since that is the hostname that will be used Test Agent Appliance to
+     # register.
+     - host: "login.netrounds-niclas-19-eks.test.netrounds.com"
+       http:
+         paths:
+           - path: /
+             backend:
+               serviceName: ncc
+               servicePort: 80
kube-system, registry-creds-ecr, Secret (v1) has been added:
- 
+ # Source: ncc-full/charts/registry-creds/templates/registry-creds-secret.yaml
+ apiVersion: v1
+ kind: Secret
+ metadata:
+   name: registry-creds-ecr
+   namespace: kube-system
+   labels:
+     app.kubernetes.io/name: registry-creds
+     helm.sh/chart: registry-creds-0.1.0
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+ type: Opaque
+ data:
+   AWS_ACCESS_KEY_ID: QUtJQUlQTVc1U0hVSlVLSk1XVUE=
+   AWS_SECRET_ACCESS_KEY: bjlUakJGZnFBeWNzTS84TzVSanFkWStZV3hZZUhpcDhlU3htSlVibA==
+   aws-account: NDY3NTYwODMwNTE1
+   aws-region: ZXUtd2VzdC0x
+   # optional role ARN to be assumed for getting ECR authorization tokens
+   aws_assume_role:
default, kubernetes-agent-daemon-py-configmap, ConfigMap (v1) has been added:
+ # Source: ncc-full/charts/ncc/templates/configmaps/agent-daemon.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   name: kubernetes-agent-daemon-py-configmap
+ data:
+   kubernetes_agent_daemon.py: |-
+     from defaults import *
+ 
+     SECRET_KEY_FILE = '/etc/netrounds/secret_key'
+     # Unique and secret string that is generated on install.
+     # This value is for example used for secure cookie handling (signed cookies),
+     # password hashing and other cryptographic operations.
+     SECRET_KEY = open(SECRET_KEY_FILE).read().strip()
+ 
+     # Configuration for access to the MySQL database.
+     DATABASES['default']['NAME'] = 'netrounds'
+     DATABASES['default']['USER'] = 'netrounds'
+     DATABASES['default']['PASSWORD'] = os.environ['MYSQL_PASSWORD']
+     DATABASES['default']['HOST'] = 'mysql.default.svc.cluster.local'
+     DATABASES['default']['PORT'] = '3306'
+ 
+     LOGGING['handlers']['console']['level'] = 'ERROR'
+     LOGGING['handlers']['console']['formatter'] = 'logstash'
+ 
+     EXTERNAL_APPS = (
+         'django.contrib.contenttypes',
+         'django.contrib.auth',
+         'django.contrib.messages',
+         'django.contrib.admin',
+         'polymorphic',
+         'django.contrib.sessions',
+         'django.contrib.staticfiles',
+         'django_countries',
+     )
+ 
+     PROJECT_APPS = (
+         'netrounds.config',
+         'netrounds.domain',
+         'netrounds.dashboard',
+         'netrounds.monitoring',
+         'netrounds.testing',
+         'netrounds.manager',
+         'netrounds.results',
+         'netrounds.genalyzer',
+         'netrounds.bbq',
+         'netrounds.apps',
+         'netrounds.utils',
+         'netrounds.subscription',
+         'netrounds.alarm',
+         'netrounds.tag',
+         'netrounds.license',
+     )
+ 
+     INSTALLED_APPS = PROJECT_APPS + EXTERNAL_APPS
+ 
+     # When GENALYZER_PROBE_AUTO_UPDATE is set to True, Test Agents with a version
+     # lower than GENALYZER_PROBE_MIN_VERSION will be automatically updated.
+     GENALYZER_PROBE_AUTO_UPDATE = False
+     GENALYZER_PROBE_MIN_VERSION = '2.6.1'
+ 
+     # The time series data is stored on disk in this directory.
+     RRD_DIR = '/var/lib/netrounds/rrd'
+ 
+     # Where the OpenVPN certificates and keys used to authenticate Test Agents
+     # are be stored.
+     OPENVPN_KEY_DIR = '/var/lib/netrounds/openvpn'
+ 
+     # How many tasks that are processed from the background task queue in parallel.
+     CALL_EXECUTER_MAX_CHILDREN = 20
+ 
+     # Whether users should be able to sign up for a new account through the GUI.
+     ALLOW_SIGNUP = False
  
+     # Whether notifications about SLA status changes should be sent to ConfD.
+     # Enabling this will require some additional processing when collecting monitor
+     # data and can thus affect performance.
+     SLA_STATUS = True
default, init-directories-sh-configmap, ConfigMap (v1) has been added:
+ # Source: ncc-full/charts/ncc/templates/configmaps/ncc.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   name: init-directories-sh-configmap
+ data:
+   init-directories.sh: |-
+     #!/bin/bash
+     mkdir -p /var/lib/netrounds/rrd
+     chown -R netrounds:netrounds /var/lib/netrounds/rrd
+     chown -R netrounds:netrounds /var/lib/netrounds/openvpn > /dev/null 2>&1
+ 
+     if [ -z "$(ls -A /var/lib/netrounds/openvpn 2> /dev/null)" ]; then
+       echo "Setting up openvpn keys."
+       /usr/bin/python -c 'from netrounds.genalyzer import easyrsa; easyrsa.clean_all(); easyrsa.build_dh(); easyrsa.build_ca(); easyrsa.build_key_server()'
+       chown -R netrounds:netrounds /var/lib/netrounds/openvpn
+       echo "Openvpn keys have been created."
+     else
+       echo "Openvpn has already been set up."
+     fi
  
+     # make sure there is a proper dh.pem symlink for the dh[1024|2048].pem
+     if [ ! -f /var/lib/netrounds/openvpn/dh.pem ]; then
+         ln -s /var/lib/netrounds/openvpn/dh*.pem /var/lib/netrounds/openvpn/dh.pem
+         chown netrounds:netrounds /var/lib/netrounds/openvpn/dh.pem --no-dereference
+     fi
default, kubernetes-ncc-no-migrations-py-configmap, ConfigMap (v1) has been added:
+ # Source: ncc-full/charts/ncc/templates/configmaps/ncc.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   name: kubernetes-ncc-no-migrations-py-configmap
+ data:
+   kubernetes_ncc_no_migrations.py: |-
+     from netrounds.settings.kubernetes_ncc import *  # NOQA
+ 
+ 
+     class DisableMigrations(object):
+ 
+         def __contains__(self, item):
+             return True
+ 
+         def __getitem__(self, item):
+             return "notmigrations"
  
+     MIGRATION_MODULES = DisableMigrations()
default, kubernetes-ncc-py-configmap, ConfigMap (v1) has been added:
+ # Source: ncc-full/charts/ncc/templates/configmaps/ncc.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   name: kubernetes-ncc-py-configmap
+ data:
+   kubernetes_ncc.py: |-
+     import os
+     from defaults import *
+ 
+     # Time series settings
+     AVAILABLE_TIME_SERIES = ['rrd', 'kafka']
+     # Set here account names which are allowed to push to kafka
+     KAFKA_ACCOUNT_WHITELIST = ['dev']
+     KAFKA_TOPIC = 'top8types'
+     KAFKA_BOOTSTRAP_SERVERS =None
+ 
+     SECRET_KEY_FILE = '/etc/netrounds/secret_key'
+ 
+     DEBUG=False
+ 
+     # Unique and secret string that is generated on install.
+     # This value is for example used for secure cookie handling (signed cookies),
+     # password hashing and other cryptographic operations.
+     SECRET_KEY = open(SECRET_KEY_FILE).read().strip()
+ 
+     # URL for the web server without trailinig slash.
+     # This is used for example when sending e-mails with links to the website.
+ 
+     SITE_URL = 'https://app.netrounds-niclas-19-eks.test.netrounds.com'
+ 
+     # Outgoing e-mails, for example sent when user requests a new password, will
+     # have this "From" header.
+     DEFAULT_FROM_EMAIL = 'Netrounds <no-reply@netrounds.com>'
+ 
+     # Email shown to the user when contact with NCC administation is required
+     CONTACT_EMAIL = 'sales@netrounds.com'
+ 
+     # How to send outgoing e-mails.
+     # It is recommended to use a local mail transfer agent (like postfix) to handle
+     # queueing and retries. Disabling all outgoing e-mails can be done by setting
+     # EMAIL_BACKEND to "django.core.mail.backends.dummy.EmailBackend".
+     # See https://docs.djangoproject.com/en/1.8/topics/email/#email-backends for
+     # more information.
+     EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'
+     EMAIL_HOST = 'email-smtp.eu-north-1.amazonaws.com'
+     EMAIL_HOST_USER = '<SES username>'
+     EMAIL_HOST_PASSWORD = os.environ['EMAIL_HOST_PASSWORD']
+     EMAIL_PORT = 587
+     EMAIL_USE_TLS = True
+ 
+     # Configuration for access to the MySQL database.
+     DATABASES['default']['NAME'] = 'netrounds'
+     DATABASES['default']['USER'] = 'netrounds'
+     DATABASES['default']['PASSWORD'] = os.environ['MYSQL_PASSWORD']
+     DATABASES['default']['HOST'] = 'mysql.default.svc.cluster.local'
+     DATABASES['default']['PORT'] = '3306'
+ 
+     # Logging config
+     # See https://docs.python.org/2/library/logging.config.html#configuration-dictionary-schema
+     # for more information.
+     #
+     # Logging level for the Netrounds processes. By default only ERROR is logged.
+     # When troubleshooting an issue, this can be changed to DEBUG to include extra logging.
+     LOGGING['handlers']['console']['level'] = 'ERROR'
+     LOGGING['handlers']['console']['formatter'] = 'logstash'
+ 
+     # Maximum length for each log tag.
+     # Log tags are information about an event that are logged along with the log
+     # message, for example the client's IP address in a HTTP request.
+     LOG_TAGS_CUTOFF = 1024
+ 
+     # Number of days a password reset link is valid.
+     PASSWORD_RESET_TIMEOUT_DAYS = 3
+ 
+     # When GENALYZER_PROBE_AUTO_UPDATE is set to True, Test Agents with a version
+     # lower than GENALYZER_PROBE_MIN_VERSION will be automatically updated.
+     GENALYZER_PROBE_AUTO_UPDATE = False
+     GENALYZER_PROBE_MIN_VERSION = '2.6.1'
+ 
+     # The time series data is stored on disk in this directory.
+     RRD_DIR = '/var/lib/netrounds/rrd'
+ 
+     # Where the OpenVPN certificates and keys used to authenticate Test Agents
+     # are be stored.
+     OPENVPN_KEY_DIR = '/var/lib/netrounds/openvpn'
+ 
+     # How many tasks that are processed from the background task queue in parallel.
+     CALL_EXECUTER_MAX_CHILDREN = 20
+ 
+     # Whether users should be able to sign up for a new account through the GUI.
+     ALLOW_SIGNUP = False
+ 
+     # Whether notifications about SLA status changes should be sent to ConfD.
+     # Enabling this will require some additional processing when collecting monitor
+     # data and can thus affect performance.
+     SLA_STATUS = True
  
+     SESSION_COOKIE_SECURE = True
+     CSFR_COOKIE_SECURE = True
default, kubernetes-probe-serviced-py-configmap, ConfigMap (v1) has been added:
+ # Source: ncc-full/charts/ncc/templates/configmaps/probe-serviced.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   name: kubernetes-probe-serviced-py-configmap
+ data:
+   kubernetes_probe_serviced.py: |-
+     from defaults import *
+ 
+     # Time series settings
+     AVAILABLE_TIME_SERIES = []
+ 
+     SECRET_KEY_FILE = '/etc/netrounds/secret_key'
+     # Unique and secret string that is generated on install.
+     # This value is for example used for secure cookie handling (signed cookies),
+     # password hashing and other cryptographic operations.
+     SECRET_KEY = open(SECRET_KEY_FILE).read().strip()
+ 
+     # Configuration for access to the MySQL database.
+     DATABASES['default']['NAME'] = 'netrounds'
+     DATABASES['default']['USER'] = 'netrounds'
+     DATABASES['default']['PASSWORD'] = os.environ['MYSQL_PASSWORD']
+     DATABASES['default']['HOST'] = 'mysql.default.svc.cluster.local'
+     DATABASES['default']['PORT'] = '3306'
+ 
+     EXTERNAL_APPS = (
+         'django.contrib.contenttypes',
+         'django.contrib.auth',
+         'django.contrib.messages',
+         'django.contrib.admin',
+         'polymorphic',
+         'django.contrib.sessions',
+         'django.contrib.staticfiles',
+         'django_countries',
+     )
+ 
+     PROJECT_APPS = (
+         'netrounds.config',
+         'netrounds.domain',
+         'netrounds.dashboard',
+         'netrounds.monitoring',
+         'netrounds.testing',
+         'netrounds.manager',
+         'netrounds.results',
+         'netrounds.genalyzer',
+         'netrounds.bbq',
+         'netrounds.apps',
+         'netrounds.utils',
+         'netrounds.subscription',
+         'netrounds.alarm',
+         'netrounds.tag',
+         'netrounds.license',
+     )
+ 
+     LOGGING['handlers']['console']['level'] = 'ERROR'
+     LOGGING['handlers']['console']['formatter'] = 'logstash'
+ 
+     INSTALLED_APPS = PROJECT_APPS + EXTERNAL_APPS
+ 
+     # When GENALYZER_PROBE_AUTO_UPDATE is set to True, Test Agents with a version
+     # lower than GENALYZER_PROBE_MIN_VERSION will be automatically updated.
+     GENALYZER_PROBE_AUTO_UPDATE = False
+     GENALYZER_PROBE_MIN_VERSION = '2.6.1'
+ 
+     # The time series data is stored on disk in this directory.
+     RRD_DIR = '/var/lib/netrounds/rrd'
+ 
+     # Where the OpenVPN certificates and keys used to authenticate Test Agents
+     # are be stored.
+     OPENVPN_KEY_DIR = '/var/lib/netrounds/openvpn'
+ 
+     # How many tasks that are processed from the background task queue in parallel.
+     CALL_EXECUTER_MAX_CHILDREN = 20
+ 
+     # Whether users should be able to sign up for a new account through the GUI.
+     ALLOW_SIGNUP = False
  
+     # Whether notifications about SLA status changes should be sent to ConfD.
+     # Enabling this will require some additional processing when collecting monitor
+     # data and can thus affect performance.
+     SLA_STATUS = True
default, registry-creds, ClusterRole (rbac.authorization.k8s.io) has been added:
- 
+ # Source: ncc-full/charts/registry-creds/templates/registry-creds-rbac.yaml
+ apiVersion: rbac.authorization.k8s.io/v1beta1
+ kind: ClusterRole
+ metadata:
+   name: registry-creds
+   labels:
+     app.kubernetes.io/name: registry-creds
+     helm.sh/chart: registry-creds-0.1.0
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+ rules:
+ - apiGroups:
+   - ""
+   resources:
+   - namespaces
+   verbs:
+   - list
+   - watch
+ - apiGroups:
+   - ""
+   resources:
+   - secrets
+   verbs:
+   - create
+   - get
+   - update
+ - apiGroups:
+   - ""
+   resources:
+   - serviceaccounts
+   verbs:
+   - get
+   - update
default, restol, Ingress (extensions) has been added:
- 
+ # Source: ncc-full/charts/restol/templates/ingress.yaml
+ apiVersion: extensions/v1beta1
+ kind: Ingress
+ metadata:
+   name: restol
+   labels:
+     app.kubernetes.io/name: restol
+     helm.sh/chart: restol-0.1.3
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+ spec:
+   tls:
+     - hosts:
+         - "app.netrounds-niclas-19-eks.test.netrounds.com"
+       secretName: ncc-tls-secret
+   rules:
+     - host: "app.netrounds-niclas-19-eks.test.netrounds.com"
+       http:
+         paths:
+           - path: /rest
+             backend:
+               serviceName: restol
+               servicePort: 80
default, email-host-password-secret, Secret (v1) has been added:
- 
+ # Source: ncc-full/charts/ncc/templates/secrets/ncc.yaml
+ apiVersion: v1
+ kind: Secret
+ metadata:
+   name: email-host-password-secret
+   labels:
+     app.kubernetes.io/name: ncc
+     helm.sh/chart: ncc-0.1.3
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+ type: Opaque
+ data:
+   emailHostPassword: PFNFUyBwYXNzd29yZD4=
default, netrounds-nossl-conf-configmap, ConfigMap (v1) has been added:
+ # Source: ncc-full/charts/ncc/templates/configmaps/ncc.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   name: netrounds-nossl-conf-configmap
+ data:
+   netrounds-nossl.conf: |-
+     <VirtualHost *:80>
+       ServerName netrounds
+       ServerAdmin support@netrounds.com
+ 
+       ## Vhost docroot
+       DocumentRoot "/usr/lib/python2.7/dist-packages/netrounds/"
+ 
+       ## Alias declarations for resources outside the DocumentRoot
+       AliasMatch ^/static/[\dabc\.]+/(.*)$ "/usr/lib/python2.7/dist-packages/netrounds/static/$1"
+       Alias /static/ "/usr/lib/python2.7/dist-packages/netrounds/static/"
+       Alias /debian "/var/www/html/debian/"
+       Alias /update "/usr/lib/python2.7/dist-packages/netrounds/static/test_agent"
+       Alias /robots.txt "/usr/lib/python2.7/dist-packages/netrounds/static/robots.txt"
+       Alias /favicon.ico "/usr/lib/python2.7/dist-packages/netrounds/static/images/favicon.ico"
+ 
+       ## Directories, there should at least be a declaration for /usr/lib/python2.7/dist-packages/netrounds/
+ 
+       <Directory "/usr/lib/python2.7/dist-packages/netrounds">
+         AllowOverride None
+         Require all granted
+       </Directory>
+ 
+       ## Logging
+       ErrorLogFormat "%M"
+       ErrorLog /dev/stdout
+       CustomLog /dev/stdout combined
+ 
+       ## Rewrite rules
+       RewriteEngine On
+ 
+       RewriteRule ^/[^/]+/speedtest(?:-flash|-websocket)?/?$ - [L]
+ 
+       RewriteRule ^/static/ - [L]
+ 
+       RewriteRule ^/debian/ - [L]
+ 
+       RewriteRule ^/update/ - [L]
  
+       WSGIDaemonProcess netrounds group=netrounds maximum-requests=1000 processes=4 threads=1 user=netrounds
+       WSGIProcessGroup netrounds
+       WSGIScriptAlias / "/usr/lib/python2.7/dist-packages/netrounds/wsgi.py"
+       WSGIPassAuthorization On
+     </VirtualHost>
default, restol-conf-configmap, ConfigMap (v1) has been added:
- 
+ # Source: ncc-full/charts/restol/templates/configmap.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   name: restol-conf-configmap
+ data:
+   restol.conf: |-
+     [netrounds-server]
+     NCC_SERVER=http://ncc.default.svc.cluster.local
+     VERIFY_SSL=0
+     SITE_URL=app.netrounds-niclas-19-eks.test.netrounds.com
+     LOG_LEVEL=ERROR
+     LOG_FORMATTER=logstash
+     RATE_LIMIT_ENABLED=0
+     RATE_LIMIT_DEFAULT=1000/minute
default, restol, Service (v1) has been added:
- 
+ # Source: ncc-full/charts/restol/templates/service.yaml
+ apiVersion: v1
+ kind: Service
+ metadata:
+   name: restol
+   labels:
+     app.kubernetes.io/name: restol
+     helm.sh/chart: restol-0.1.3
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+ spec:
+   type: ClusterIP
+   ports:
+     - port: 80
+       targetPort: 80
+       protocol: TCP
+       name: restol
+   selector:
+     app.kubernetes.io/name: restol
+     app.kubernetes.io/instance: ncc-full
default, mysql, Secret (v1) has been added:
- 
+ # Source: ncc-full/charts/mysql/templates/secrets.yaml
+ apiVersion: v1
+ kind: Secret
+ metadata:
+   name: mysql
+   labels:
+     app: mysql
+     chart: "mysql-0.10.2"
+     release: "ncc-full"
+     heritage: "Tiller"
+ type: Opaque
+ data:
+   
+   mysql-root-password:  "bmV0cm91bmRz"
+   
+   
+   mysql-password:  "bmV0cm91bmRz"
default, setup-database-sh-configmap, ConfigMap (v1) has been added:
+ # Source: ncc-full/charts/ncc/templates/configmaps/ncc.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   name: setup-database-sh-configmap
+ data:
+   setup-database.sh: |-
+     #!/bin/bash
+     set -e
+     while ! mysqladmin ping -h"$MYSQL_HOST" --connect_timeout=2 --silent; do
+       echo "Waiting for mysql to be ready" && sleep 2s
+     done
+ 
+     echo "MySQL is ready - setting up database"
+     echo "Running django migrations is a process that will take some time."
+     echo "You may notice that process freezed on 'Running migrations:' operation."
+     echo "This is completely normal and should not be interrupted."
+ 
+     # Migrate
+     echo "Running migrations."
+     django-admin migrate --noinput
+     echo "Migrations applied successfully."
+ 
+     # Create cache table
+     echo "Creating cache table."
+     django-admin createcachetable
+     echo "Cache table has been created successfully."
+ 
+     # Sync test scripts
+     echo "Synchronize test scripts."
+     django-admin synctestscripts /usr/lib/netrounds/test-scripts/
+     echo "Test scripts synchronized successfully."
+ 
+     # When -e is set in this section then script fails and users are not being created.
+     set +e
+     # Create default user.
+     is_default_user=$(django-admin dumpdata auth.User | grep \"$DEFAULT_USER_NAME\")
+     if [ -z "$is_default_user" ]; then
+       echo "Creating default user."
+       django-admin user_create $DEFAULT_USER_NAME --password $DEFAULT_USER_PASSWORD --superuser --staff
+       echo "Default user has been created successfully."
+     fi
+ 
+     # Create default account.
+     is_default_account=$(django-admin dumpdata domain.Domain | grep \"$DEFAULT_ACCOUNT\")
+     if [ -z "$is_default_account" ]; then
+       echo "Creating default account."
+       django-admin account_create --owner $DEFAULT_USER_NAME --name $DEFAULT_ACCOUNT $DEFAULT_ACCOUNT
+       echo "Default account has been created successfully."
+     fi
+ 
+     # create Confd user
+     if [ "$CONFD_ENABLED" = true ]; then
+       is_confd_user=$(django-admin dumpdata auth.User | grep \"$CONFD_USER\")
+       if [ -z "$is_confd_user" ]; then
+         echo "Creating confd user."
+         django-admin user_create $CONFD_USER --password=$CONFD_PASSWORD
+         echo "Confd user has been created successfully."
+       fi
+     fi
+     echo "All database objects have been created."
  
+     set -e
+     echo "Activating license."
+     django-admin license activate /usr/lib/python2.7/dist-packages/netrounds/license.txt
+     echo "License has been activated successfully."
default, registry-creds, ClusterRoleBinding (rbac.authorization.k8s.io) has been added:
- 
+ # Source: ncc-full/charts/registry-creds/templates/registry-creds-rbac.yaml
+ apiVersion: rbac.authorization.k8s.io/v1beta1
+ kind: ClusterRoleBinding
+ metadata:
+   name: registry-creds
+   labels:
+     app.kubernetes.io/name: registry-creds
+     helm.sh/chart: registry-creds-0.1.0
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+ roleRef:
+   apiGroup: rbac.authorization.k8s.io
+   kind: ClusterRole
+   name: registry-creds
+ subjects:
+   - kind: ServiceAccount
+     name: registry-creds
+     namespace:  kube-system
default, ncc-openvpn, Service (v1) has been added:
- 
+ # Source: ncc-full/charts/ncc/templates/openvpn-service.yaml
+ apiVersion: v1
+ kind: Service
+ metadata:
+   name: ncc-openvpn
+   labels:
+     app.kubernetes.io/name: ncc
+     helm.sh/chart: ncc-0.1.3
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+   annotations:
+     external-dns.alpha.kubernetes.io/hostname: login.netrounds-niclas-19-eks.test.netrounds.com
+     
+ spec:
+   selector:
+     app.kubernetes.io/name: ncc
+     app.kubernetes.io/instance: ncc-full
+   type: LoadBalancer
+   ports:
+     - name: openvpn
+       protocol: TCP
+       targetPort: 6000
+       port: 443
default, mysql, Deployment (extensions) has been added:
- 
+ # Source: ncc-full/charts/mysql/templates/deployment.yaml
+ apiVersion: extensions/v1beta1
+ kind: Deployment
+ metadata:
+   name: mysql
+   labels:
+     app: mysql
+     chart: "mysql-0.10.2"
+     release: "ncc-full"
+     heritage: "Tiller"
+ spec:
+   template:
+     metadata:
+       labels:
+         app: mysql
+     spec:
+       initContainers:
+       - name: "remove-lost-found"
+         image: "busybox:1.25.0"
+         imagePullPolicy: "IfNotPresent"
+         command:  ["rm", "-fr", "/var/lib/mysql/lost+found"]
+         volumeMounts:
+         - name: data
+           mountPath: /var/lib/mysql
+       # - name: do-something
+       #   image: busybox
+       #   command: ['do', 'something']
+       
+       containers:
+       - name: mysql
+         image: "mysql:5.7.23"
+         imagePullPolicy: "IfNotPresent"
+         resources:
+           requests:
+             cpu: 100m
+             memory: 256Mi
+           
+         env:
+         - name: MYSQL_ROOT_PASSWORD
+           valueFrom:
+             secretKeyRef:
+               name: mysql
+               key: mysql-root-password
+         - name: MYSQL_PASSWORD
+           valueFrom:
+             secretKeyRef:
+               name: mysql
+               key: mysql-password
+         - name: MYSQL_USER
+           value: "netrounds"
+         - name: MYSQL_DATABASE
+           value: "netrounds"
+         ports:
+         - name: mysql
+           containerPort: 3306
+         livenessProbe:
+           exec:
+             command:
+             - sh
+             - -c
+             - "mysqladmin ping -u root -p${MYSQL_ROOT_PASSWORD}"
+           initialDelaySeconds: 30
+           periodSeconds: 10
+           timeoutSeconds: 5
+           successThreshold: 1
+           failureThreshold: 3
+         readinessProbe:
+           exec:
+             command:
+             - sh
+             - -c
+             - "mysqladmin ping -u root -p${MYSQL_ROOT_PASSWORD}"
+           initialDelaySeconds: 5
+           periodSeconds: 10
+           timeoutSeconds: 1
+           successThreshold: 1
+           failureThreshold: 3
+         volumeMounts:
+         - name: data
+           mountPath: /var/lib/mysql
+         # - name: extras
+         #   mountPath: /usr/share/extras
+         #   readOnly: true
+         
+       volumes:
+       - name: data
+         persistentVolumeClaim:
+           claimName: ncc-objects-mysql-storage
+       # - name: extras
+       #   emptyDir: {}
default, default-user-password-secret, Secret (v1) has been added:
- 
+ # Source: ncc-full/charts/ncc/templates/secrets/ncc.yaml
+ apiVersion: v1
+ kind: Secret
+ metadata:
+   name: default-user-password-secret
+   labels:
+     app.kubernetes.io/name: ncc
+     helm.sh/chart: ncc-0.1.3
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+ type: Opaque
+ data:
+   defaultUserPassword: UGFzc3dvcmQx
default, kubernetes-call-executer-py-configmap, ConfigMap (v1) has been added:
+ # Source: ncc-full/charts/ncc/templates/configmaps/call-executer.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   name: kubernetes-call-executer-py-configmap
+ data:
+   kubernetes_call_executer.py: |-
+     from defaults import *
+ 
+     # Time series settings
+     AVAILABLE_TIME_SERIES = ['rrd', 'kafka']
+     # Set here account names which are allowed to push to kafka
+     KAFKA_ACCOUNT_WHITELIST = ['dev']
+     KAFKA_TOPIC = 'top8types'
+     KAFKA_BOOTSTRAP_SERVERS =None
+ 
+     SECRET_KEY_FILE = '/etc/netrounds/secret_key'
+     # Unique and secret string that is generated on install.
+     # This value is for example used for secure cookie handling (signed cookies),
+     # password hashing and other cryptographic operations.
+     SECRET_KEY = open(SECRET_KEY_FILE).read().strip()
+ 
+     # Configuration for access to the MySQL database.
+     DATABASES['default']['NAME'] = 'netrounds'
+     DATABASES['default']['USER'] = 'netrounds'
+     DATABASES['default']['PASSWORD'] = os.environ['MYSQL_PASSWORD']
+     DATABASES['default']['HOST'] = 'mysql.default.svc.cluster.local'
+     DATABASES['default']['PORT'] = '3306'
+ 
+     LOGGING['handlers']['console']['level'] = 'ERROR'
+     LOGGING['handlers']['console']['formatter'] = 'logstash'
+ 
+     EXTERNAL_APPS = (
+         'django.contrib.contenttypes',
+         'django.contrib.auth',
+         'django.contrib.messages',
+         'django.contrib.admin',
+         'polymorphic',
+         'django.contrib.sessions',
+         'django.contrib.staticfiles',
+         'django_countries',
+     )
+ 
+     PROJECT_APPS = (
+         'netrounds.config',
+         'netrounds.domain',
+         'netrounds.dashboard',
+         'netrounds.monitoring',
+         'netrounds.testing',
+         'netrounds.manager',
+         'netrounds.results',
+         'netrounds.genalyzer',
+         'netrounds.bbq',
+         'netrounds.apps',
+         'netrounds.utils',
+         'netrounds.subscription',
+         'netrounds.alarm',
+         'netrounds.tag',
+         'netrounds.license',
+     )
+ 
+     INSTALLED_APPS = PROJECT_APPS + EXTERNAL_APPS
+ 
+     # When GENALYZER_PROBE_AUTO_UPDATE is set to True, Test Agents with a version
+     # lower than GENALYZER_PROBE_MIN_VERSION will be automatically updated.
+     GENALYZER_PROBE_AUTO_UPDATE = False
+     GENALYZER_PROBE_MIN_VERSION = '2.6.1'
+ 
+     # The time series data is stored on disk in this directory.
+     RRD_DIR = '/var/lib/netrounds/rrd'
+ 
+     # Where the OpenVPN certificates and keys used to authenticate Test Agents
+     # are be stored.
+     OPENVPN_KEY_DIR = '/var/lib/netrounds/openvpn'
+ 
+     # How many tasks that are processed from the background task queue in parallel.
+     CALL_EXECUTER_MAX_CHILDREN = 20
+ 
+     # Whether users should be able to sign up for a new account through the GUI.
+     ALLOW_SIGNUP = False
  
+     # Whether notifications about SLA status changes should be sent to ConfD.
+     # Enabling this will require some additional processing when collecting monitor
+     # data and can thus affect performance.
+     SLA_STATUS = True
default, mysql-test, ConfigMap (v1) has been added:
- 
+ # Source: ncc-full/charts/mysql/templates/tests/test-configmap.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   name: mysql-test
+   labels:
+     app: mysql
+     chart: "mysql-0.10.2"
+     heritage: "Tiller"
+     release: "ncc-full"
+ data:
+   run.sh: |-
+     @test "Testing MySQL Connection" {
+       mysql --host=mysql --port=3306 -u root -pnetrounds
+     }
kube-system, registry-creds, ServiceAccount (v1) has been added:
- 
+ # Source: ncc-full/charts/registry-creds/templates/registry-creds-rbac.yaml
+ apiVersion: v1
+ kind: ServiceAccount
+ metadata:
+   name: registry-creds
+   namespace: kube-system
+   labels:
+     app.kubernetes.io/name: registry-creds
+     helm.sh/chart: registry-creds-0.1.0
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
kube-system, registry-creds, Deployment (extensions) has been added:
- 
+ # Source: ncc-full/charts/registry-creds/templates/registry-creds-deployment.yaml
+ apiVersion: extensions/v1beta1
+ kind: Deployment
+ metadata:
+   name: registry-creds
+   namespace: kube-system
+   labels:
+     app.kubernetes.io/name: registry-creds
+     helm.sh/chart: registry-creds-0.1.0
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+ spec:
+   replicas: 1
+   selector:
+     matchLabels:
+       app.kubernetes.io/name: registry-creds
+       app.kubernetes.io/instance: ncc-full
+   template:
+     metadata:
+       labels:
+         app.kubernetes.io/name: registry-creds
+         app.kubernetes.io/instance: ncc-full
+     spec:
+       serviceAccountName: registry-creds
+       containers:
+       - name: registry-creds
+         image: "upmcenterprises/registry-creds:1.9"
+         imagePullPolicy: IfNotPresent
+         env:          
+         - name: AWS_ACCESS_KEY_ID
+           valueFrom:
+             secretKeyRef:
+               name: registry-creds-ecr
+               key: AWS_ACCESS_KEY_ID
+         - name: AWS_SECRET_ACCESS_KEY
+           valueFrom:
+             secretKeyRef:
+               name: registry-creds-ecr
+               key: AWS_SECRET_ACCESS_KEY
+         - name: awsaccount
+           valueFrom:
+             secretKeyRef:
+               name: registry-creds-ecr
+               key: aws-account
+         - name: awsregion
+           valueFrom:
+             secretKeyRef:
+               name: registry-creds-ecr
+               key: aws-region
+         - name: aws_assume_role
+           valueFrom:
+             secretKeyRef:
+               name: registry-creds-ecr
+               key: aws_assume_role
+         resources:
+           {}
default, agent-ws-server, Ingress (extensions) has been added:
- 
+ # Source: ncc-full/charts/agent-ws-server/templates/ingress.yaml
+ apiVersion: extensions/v1beta1
+ kind: Ingress
+ metadata:
+   name: agent-ws-server
+   labels:
+     app.kubernetes.io/name: agent-ws-server
+     helm.sh/chart: agent-ws-server-0.1.3
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+   annotations:
+     kubernetes.io/ingress.class: nginx
+     
+ spec:
+   tls:
+     - hosts:
+         - "agent.netrounds-niclas-19-eks.test.netrounds.com"
+       secretName: agent-tls-secret
+   rules:
+     - host: "agent.netrounds-niclas-19-eks.test.netrounds.com"
+       http:
+         paths:
+           - path: /
+             backend:
+               serviceName: agent-ws-server
+               servicePort: 9200
default, netrounds-iptables-setup-sh-configmap, ConfigMap (v1) has been added:
+ # Source: ncc-full/charts/ncc/templates/configmaps/openvpn.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   name: netrounds-iptables-setup-sh-configmap
+ data:
+   netrounds-iptables-setup.sh: |-
+     #!/bin/bash
+ 
+     # Info from openvpn documentation:
+     #     For --dev tun execute as:
+     #     cmd tun_dev tun_mtu link_mtu ifconfig_local_ip ifconfig_remote_ip [ init | restart ]
+ 
+     TUN_IF=$dev
+ 
+     # Set default values
+     : ${PROBE_SERVICED_PORT:=4334}
+     : ${UPDATE_HTTP_PORT:=80}
+     # # Connections from probes to probeserviced
+     iptables -A INPUT -i "$TUN_IF" -p tcp --dport "$PROBE_SERVICED_PORT" -j ACCEPT
+ 
+     # # Connections from probes to webapp
+     iptables -A INPUT -i "$TUN_IF" -p tcp --dport "$UPDATE_HTTP_PORT" -j ACCEPT
+ 
+     # # Allow ESTABLISHED and RELATED from probes
+     iptables -A INPUT -i "$TUN_IF" -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT
+ 
+     # # Drop everything else
+     iptables -A INPUT -i "$TUN_IF"  -j DROP
  
+     # # Start up the probe login service
+     if [ "$IS_DEVENV" = true ] ; then
+         # Nothing yet
+         :
+     else
+         mkdir -p /var/lib/openvpn/
+         echo "ifconfig_local=$ifconfig_local" > /var/lib/openvpn/netrounds.env
+     fi
default, kubernetes-service-monitor-py-configmap, ConfigMap (v1) has been added:
+ # Source: ncc-full/charts/ncc/templates/configmaps/service-monitor.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   name: kubernetes-service-monitor-py-configmap
+ data:
+   kubernetes_service_monitor.py: |-
+     from defaults import *
+ 
+     # Time series settings
+     AVAILABLE_TIME_SERIES = ['rrd', 'kafka']
+     # Set here account names which are allowed to push to kafka
+     KAFKA_ACCOUNT_WHITELIST = ['dev']
+     KAFKA_TOPIC = 'top8types'
+     KAFKA_BOOTSTRAP_SERVERS =None
+ 
+     SECRET_KEY_FILE = '/etc/netrounds/secret_key'
+     # Unique and secret string that is generated on install.
+     # This value is for example used for secure cookie handling (signed cookies),
+     # password hashing and other cryptographic operations.
+     SECRET_KEY = open(SECRET_KEY_FILE).read().strip()
+ 
+     # Configuration for access to the MySQL database.
+     DATABASES['default']['NAME'] = 'netrounds'
+     DATABASES['default']['USER'] = 'netrounds'
+     DATABASES['default']['PASSWORD'] = os.environ['MYSQL_PASSWORD']
+     DATABASES['default']['HOST'] = 'mysql.default.svc.cluster.local'
+     DATABASES['default']['PORT'] = '3306'
+ 
+     LOGGING['handlers']['console']['level'] = 'ERROR'
+     LOGGING['handlers']['console']['formatter'] = 'logstash'
+ 
+     EXTERNAL_APPS = (
+         'django.contrib.contenttypes',
+         'django.contrib.auth',
+         'django.contrib.messages',
+         'django.contrib.admin',
+         'polymorphic',
+         'django.contrib.sessions',
+         'django.contrib.staticfiles',
+         'django_countries',
+     )
+ 
+     PROJECT_APPS = (
+         'netrounds.config',
+         'netrounds.domain',
+         'netrounds.dashboard',
+         'netrounds.monitoring',
+         'netrounds.testing',
+         'netrounds.manager',
+         'netrounds.results',
+         'netrounds.genalyzer',
+         'netrounds.bbq',
+         'netrounds.apps',
+         'netrounds.utils',
+         'netrounds.subscription',
+         'netrounds.alarm',
+         'netrounds.tag',
+         'netrounds.license',
+     )
+ 
+     INSTALLED_APPS = PROJECT_APPS + EXTERNAL_APPS
+ 
+     # When GENALYZER_PROBE_AUTO_UPDATE is set to True, Test Agents with a version
+     # lower than GENALYZER_PROBE_MIN_VERSION will be automatically updated.
+     GENALYZER_PROBE_AUTO_UPDATE = False
+     GENALYZER_PROBE_MIN_VERSION = '2.6.1'
+ 
+     # The time series data is stored on disk in this directory.
+     RRD_DIR = '/var/lib/netrounds/rrd'
+ 
+     # Where the OpenVPN certificates and keys used to authenticate Test Agents
+     # are be stored.
+     OPENVPN_KEY_DIR = '/var/lib/netrounds/openvpn'
+ 
+     # How many tasks that are processed from the background task queue in parallel.
+     CALL_EXECUTER_MAX_CHILDREN = 20
+ 
+     # Whether users should be able to sign up for a new account through the GUI.
+     ALLOW_SIGNUP = False
  
+     # Whether notifications about SLA status changes should be sent to ConfD.
+     # Enabling this will require some additional processing when collecting monitor
+     # data and can thus affect performance.
+     SLA_STATUS = True
default, ncc, Service (v1) has been added:
- 
+ # Source: ncc-full/charts/ncc/templates/service.yaml
+ apiVersion: v1
+ kind: Service
+ metadata:
+   name: ncc
+   labels:
+     app.kubernetes.io/name: ncc
+     helm.sh/chart: ncc-0.1.3
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+ spec:
+   type: ClusterIP
+   ports:
+     - port: 80
+       targetPort: 80
+       name: ncc
+       protocol: TCP
+     - port: 8000
+       targetPort: 8000
+       name: service-monitor
+       protocol: TCP
+     - port: 9100
+       targetPort: 9100
+       name: agent-daemon
+       protocol: TCP
+     - port: 6000
+       targetPort: 6000
+       name: openvpn
+       protocol: TCP
+     - port: 4334
+       targetPort: 4334
+       name: probe-serviced
+       protocol: TCP
+   selector:
+     app.kubernetes.io/name: ncc
+     app.kubernetes.io/instance: ncc-full
default, ncc, Deployment (apps) has been added:
+ # Source: ncc-full/charts/ncc/templates/deployment.yaml
+ apiVersion: apps/v1beta2
+ kind: Deployment
+ metadata:
+   name: ncc
+   labels:
+     app.kubernetes.io/name: ncc
+     helm.sh/chart: ncc-0.1.3
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+ spec:
+   selector:
+     matchLabels:
+       app.kubernetes.io/name: ncc
+       app.kubernetes.io/instance: ncc-full
+   serviceName: ncc
+   replicas: 1
+   template:
+     metadata:
+       labels:
+         app.kubernetes.io/name: ncc
+         app.kubernetes.io/instance: ncc-full
+     spec:
+       # due to a django bug, database migrations may fail to make any
+       # visible progress for a looong time
+       progressDeadlineSeconds: 1800
+       volumes:
+         - name: kubernetes-ncc-no-migrations-py-configmap
+           configMap:
+             name: kubernetes-ncc-no-migrations-py-configmap
+         - name: kubernetes-ncc-py-configmap
+           configMap:
+             name: kubernetes-ncc-py-configmap
+             defaultMode: 0777
+         - name: kubernetes-agent-daemon-py-configmap
+           configMap:
+             name: kubernetes-agent-daemon-py-configmap
+         - name: kubernetes-call-executer-py-configmap
+           configMap:
+             name: kubernetes-call-executer-py-configmap
+         - name: kubernetes-probe-serviced-py-configmap
+           configMap:
+             name: kubernetes-probe-serviced-py-configmap
+         - name: kubernetes-service-monitor-py-configmap
+           configMap:
+             name: kubernetes-service-monitor-py-configmap
+         - name: netrounds-conf-configmap
+           configMap:
+             name: netrounds-conf-configmap
+         - name: netrounds6-conf-configmap
+           configMap:
+             name: netrounds6-conf-configmap
+         - name: netrounds-iptables-setup-sh-configmap
+           configMap:
+             name: netrounds-iptables-setup-sh-configmap
+             defaultMode: 0744
+         - name: init-directories-sh-configmap
+           configMap:
+             name: init-directories-sh-configmap
+             defaultMode: 0744
+         - name: setup-database-sh-configmap
+           configMap:
+             name: setup-database-sh-configmap
+             defaultMode: 0744
+         - name: probe-connect-conf-secret
+           secret:
+             secretName: probe-connect-conf-secret
+         - name: email-host-password-secret
+           secret:
+             secretName: email-host-password-secret
+         - name: data
+           persistentVolumeClaim:
+             claimName: ncc-objects-ncc-storage
+         - name: netrounds-nossl-conf-configmap
+           configMap:
+             name: netrounds-nossl-conf-configmap
+             defaultMode: 0744
+         - name: license-txt-secret
+           secret:
+             secretName: license-txt-secret
+       imagePullSecrets:
+       - name: awsecr-cred
+       
+       initContainers:
+         - name: setup-database
+           image: "467560830515.dkr.ecr.eu-west-1.amazonaws.com/netrounds/ncc:2.28.0"
+           imagePullPolicy: IfNotPresent
+           command: ["/usr/lib/netrounds/setup-database.sh"]
+           env:
+             - name: DJANGO_SETTINGS_MODULE
+               value: netrounds.settings.kubernetes_ncc
+             - name: MYSQL_HOST
+               value: mysql.default.svc.cluster.local
+             - name: MYSQL_PASSWORD
+               valueFrom:
+                 secretKeyRef:
+                   name: mysql
+                   key: mysql-password
+             - name: CONFD_ENABLED
+               value: "false"
+             - name: DEFAULT_USER_NAME
+               value: dev@netrounds.com
+             - name: DEFAULT_USER_PASSWORD
+               valueFrom:
+                 secretKeyRef:
+                   name: default-user-password-secret
+                   key: defaultUserPassword
+             - name: DEFAULT_ACCOUNT
+               value: dev
+             - name: EMAIL_HOST_PASSWORD
+               valueFrom:
+                 secretKeyRef:
+                   name: email-host-password-secret
+                   key: emailHostPassword
+             - name: EMAIL_HOST_PASSWORD
+               valueFrom:
+                 secretKeyRef:
+                   name: email-host-password-secret
+                   key: emailHostPassword
+           volumeMounts:
+             - name: license-txt-secret
+               mountPath: /usr/lib/python2.7/dist-packages/netrounds/license.txt
+               subPath: license.txt
+             - name: data
+               mountPath: /var/lib/netrounds
+             - name: init-directories-sh-configmap
+               mountPath: /usr/lib/netrounds/init-directories.sh
+               subPath: init-directories.sh
+             - name: setup-database-sh-configmap
+               mountPath: /usr/lib/netrounds/setup-database.sh
+               subPath: setup-database.sh
+             - name: kubernetes-ncc-py-configmap
+               mountPath: /usr/lib/python2.7/dist-packages/netrounds/settings/kubernetes_ncc.py
+               subPath: kubernetes_ncc.py
+             - name: kubernetes-ncc-no-migrations-py-configmap
+               mountPath: /usr/lib/python2.7/dist-packages/netrounds/settings/kubernetes_ncc_no_migrations.py
+               subPath: kubernetes_ncc_no_migrations.py
+         - name: init-certificates
+           image: "467560830515.dkr.ecr.eu-west-1.amazonaws.com/netrounds/ncc:2.28.0"
+           imagePullPolicy: IfNotPresent
+           command: ["/usr/lib/netrounds/init-directories.sh"]
+           env:
+             - name: DJANGO_SETTINGS_MODULE
+               value: netrounds.settings.kubernetes_ncc
+             - name: MYSQL_PASSWORD
+               valueFrom:
+                 secretKeyRef:
+                   name: mysql
+                   key: mysql-password
+             - name: EMAIL_HOST_PASSWORD
+               valueFrom:
+                 secretKeyRef:
+                   name: email-host-password-secret
+                   key: emailHostPassword
+           volumeMounts:
+             - name: data
+               mountPath: /var/lib/netrounds
+             - name: init-directories-sh-configmap
+               mountPath: /usr/lib/netrounds/init-directories.sh
+               subPath: init-directories.sh
+             - name: kubernetes-ncc-py-configmap
+               mountPath: /usr/lib/python2.7/dist-packages/netrounds/settings/kubernetes_ncc.py
+               subPath: kubernetes_ncc.py
+             - name: kubernetes-ncc-no-migrations-py-configmap
+               mountPath: /usr/lib/python2.7/dist-packages/netrounds/settings/kubernetes_ncc_no_migrations.py
+               subPath: kubernetes_ncc_no_migrations.py
+       containers:
+         # NCC container
+         - name: ncc
+           image: "467560830515.dkr.ecr.eu-west-1.amazonaws.com/netrounds/ncc:2.28.0"
+           imagePullPolicy: IfNotPresent
+           livenessProbe:
+             httpGet:
+               path: /
+               port: 80
+             initialDelaySeconds: 10
+             periodSeconds: 10
+           readinessProbe:
+             httpGet:
+               path: /
+               port: 80
+             initialDelaySeconds: 10
+             periodSeconds: 10
+           volumeMounts:
+             - name: kubernetes-ncc-no-migrations-py-configmap
+               mountPath: /usr/lib/python2.7/dist-packages/netrounds/settings/kubernetes_ncc_no_migrations.py
+               subPath: kubernetes_ncc_no_migrations.py
+             - name: kubernetes-ncc-py-configmap
+               mountPath: /usr/lib/python2.7/dist-packages/netrounds/settings/kubernetes_ncc.py
+               subPath: kubernetes_ncc.py
+             - name: netrounds-nossl-conf-configmap
+               mountPath: /etc/apache2/sites-available/netrounds-nossl.conf
+               subPath: netrounds-nossl.conf
+             - name: data
+               mountPath: /var/lib/netrounds
+           ports:
+             - name: ncc
+               containerPort: 80
+           env:
+             - name: DJANGO_SETTINGS_MODULE
+               value: netrounds.settings.kubernetes_ncc
+             - name: MYSQL_PASSWORD
+               valueFrom:
+                 secretKeyRef:
+                   name: mysql
+                   key: mysql-password
+             - name: EMAIL_HOST_PASSWORD
+               valueFrom:
+                 secretKeyRef:
+                   name: email-host-password-secret
+                   key: emailHostPassword
+         # Agent daemon container
+         - name: agent-daemon
+           image: "467560830515.dkr.ecr.eu-west-1.amazonaws.com/netrounds/agent-daemon:2.28.0"
+           imagePullPolicy: IfNotPresent
+           readinessProbe:
+             tcpSocket:
+               port: 9100
+             initialDelaySeconds: 10
+             periodSeconds: 10
+           volumeMounts:
+             - name: kubernetes-agent-daemon-py-configmap
+               mountPath: /usr/lib/python2.7/dist-packages/netrounds/settings/kubernetes_agent_daemon.py
+               subPath: kubernetes_agent_daemon.py
+           ports:
+             - name: agent-daemon
+               containerPort: 9100
+           env:
+             - name: DJANGO_SETTINGS_MODULE
+               value: netrounds.settings.kubernetes_agent_daemon
+             - name: TA_AGENT_DAEMON_LISTEN_PORT
+               value: "9100"
+             - name: TA_AGENT_DAEMON_TRANSPORT
+               value: tcp
+             - name: TA_AGENT_DAEMON_LISTEN_INTERFACE
+               value: "*"
+             - name: TA_AGENT_DAEMON_WEBSOCKET_TIMEOUT
+               value: "60"
+             - name: MYSQL_PASSWORD
+               valueFrom:
+                 secretKeyRef:
+                   name: mysql
+                   key: mysql-password
+ 
+         # Call executer container
+         - name: call-executer
+           image: "467560830515.dkr.ecr.eu-west-1.amazonaws.com/netrounds/call-executer:2.28.0"
+           imagePullPolicy: IfNotPresent
+           volumeMounts:
+             - name: kubernetes-call-executer-py-configmap
+               mountPath: /usr/lib/python2.7/dist-packages/netrounds/settings/kubernetes_call_executer.py
+               subPath: kubernetes_call_executer.py
+             - name: data
+               mountPath: /var/lib/netrounds
+           env:
+             - name: DJANGO_SETTINGS_MODULE
+               value: netrounds.settings.kubernetes_call_executer
+             - name: MYSQL_PASSWORD
+               valueFrom:
+                 secretKeyRef:
+                   name: mysql
+                   key: mysql-password
+ 
+         # Probe serviced container
+         - name: probe-serviced
+           image: "467560830515.dkr.ecr.eu-west-1.amazonaws.com/netrounds/probe-serviced:2.28.0"
+           imagePullPolicy: IfNotPresent
+           readinessProbe:
+             tcpSocket:
+               port: 4334
+             initialDelaySeconds: 10
+             periodSeconds: 10
+           volumeMounts:
+             - name: kubernetes-probe-serviced-py-configmap
+               mountPath: /usr/lib/python2.7/dist-packages/netrounds/settings/kubernetes_probe_serviced.py
+               subPath: kubernetes_probe_serviced.py
+           ports:
+             - name: probe-serviced
+               containerPort: 4334
+               protocol: TCP
+           env:
+             - name: DJANGO_SETTINGS_MODULE
+               value: netrounds.settings.kubernetes_probe_serviced
+             - name: MYSQL_PASSWORD
+               valueFrom:
+                 secretKeyRef:
+                   name: mysql
+                   key: mysql-password
  
+         # Openvpn container
+         - name: openvpn
+           image: "467560830515.dkr.ecr.eu-west-1.amazonaws.com/netrounds/openvpn:2.28.0"
+           imagePullPolicy: IfNotPresent
+           livenessProbe:
+             tcpSocket:
+               port: 6000
+             initialDelaySeconds: 10
+             periodSeconds: 10
+           readinessProbe:
+             tcpSocket:
+               port: 6000
+             initialDelaySeconds: 10
+             periodSeconds: 10
+           volumeMounts:
+             - name: netrounds-conf-configmap
+               mountPath: /etc/openvpn/netrounds.conf
+               subPath: netrounds.conf
+             - name: netrounds6-conf-configmap
+               mountPath: /etc/openvpn/netrounds6.conf
+               subPath: netrounds6.conf
+             - name: netrounds-iptables-setup-sh-configmap
+               mountPath: /etc/openvpn/netrounds-iptables-setup.sh
+               subPath: netrounds-iptables-setup.sh
+             - name: data
+               mountPath: /var/lib/netrounds
+             - name: probe-connect-conf-secret
+               mountPath: /etc/netrounds/probe-connect.conf
+               subPath: probe-connect.conf
+           ports:
+             - name: openvpn
+               containerPort: 6000
+               protocol: TCP
+           securityContext:
+             capabilities:
+               add:
+                 - NET_ADMIN
+           env:
+             - name: DJANGO_SETTINGS_MODULE
+               value: netrounds.settings.kubernetes_openvpn
+         # fsfreeze container
+         - name: fsfreeze
+           image: "467560830515.dkr.ecr.eu-west-1.amazonaws.com/netrounds/fsfreeze:2.28.0"
+           imagePullPolicy: IfNotPresent
+           command:
+             - /bin/sh
+           args:
+             - -c
+             - while true; do touch /tmp/healthy; sleep 60; done
+           livenessProbe:
+             exec:
+               command:
+                 - cat
+                 - /tmp/healthy
+             initialDelaySeconds: 5
+             periodSeconds: 5
+           readinessProbe:
+             exec:
+               command:
+                 - cat
+                 - /tmp/healthy
+             initialDelaySeconds: 5
+             periodSeconds: 5
+           volumeMounts:
+             - name: data
+               mountPath: /var/lib/netrounds
+           securityContext:
+             privileged: true
+         # Service monitor container
+         - name: service-monitor
+           image: "467560830515.dkr.ecr.eu-west-1.amazonaws.com/netrounds/service-monitor:2.28.0"
+           imagePullPolicy: IfNotPresent
+           readinessProbe:
+             tcpSocket:
+               port: 8000
+             initialDelaySeconds: 10
+             periodSeconds: 10
+           livenessProbe:
+             httpGet:
+               path: /
+               port: 8000
+             initialDelaySeconds: 10
+             periodSeconds: 10
+           volumeMounts:
+             - name: kubernetes-service-monitor-py-configmap
+               mountPath: /usr/lib/python2.7/dist-packages/netrounds/settings/kubernetes_service_monitor.py
+               subPath: kubernetes_service_monitor.py
+           ports:
+             - name: service-monitor
+               containerPort: 8000
+               protocol: TCP
+           env:
+             - name: DJANGO_SETTINGS_MODULE
+               value: netrounds.settings.kubernetes_service_monitor
+             - name: MYSQL_PASSWORD
+               valueFrom:
+                 secretKeyRef:
+                   name: mysql
+                   key: mysql-password
default, license-txt-secret, Secret (v1) has been added:
- 
+ # Source: ncc-full/charts/ncc/templates/secrets/ncc.yaml
+ apiVersion: v1
+ kind: Secret
+ metadata:
+   name: license-txt-secret
+   labels:
+     app.kubernetes.io/name: ncc
+     helm.sh/chart: ncc-0.1.3
+     app.kubernetes.io/instance: ncc-full
+     app.kubernetes.io/managed-by: Tiller
+ type: Opaque
+ data:
+   license.txt: LS0tLS1CRUdJTiBORVRST1VORFMgTElDRU5TRS0tLS0tCjdCMjI2MzZmNmQ2ZDY1NmU3NDIyM0EyMjU0Njg2OTczMjA2YzY5NjM2NTZlNzM2NTIwNjk3MzIwNmY2ZTZjNzkKMjA2MTZjNmM2Zjc3NjU2NDIwNzQ2ZjIwNjI2NTIwNzU3MzY1NjQyMDYyNzkyMDY1NmQ3MDZjNmY3OTY1NjU3MwoyMDYxNzQyMDRlNjU3NDcyNmY3NTZlNjQ3MzIwNjQ3NTcyNjk2ZTY3MjA2NDY1NzY2NTZjNmY3MDZkNjU2ZTc0CjIwNjE2ZTY0MjA3NDY1NzM3NDY5NmU2NzJlMjIyQzIyNjQ2OTczNjE2MjZjNjU1ZjYxNjY3NDY1NzIyMjNBMjIKMzIzMDMyMzAyZDMwMzIyZDMwMzEyMjJDMjI2NDY5NzM2MTYyNmM2NTY0NWY2MjY1NjY2ZjcyNjUyMjNBMjIzMgozMDMxMzgyZDMxMzIyZDMwMzEyMjJDMjI3NjY5NmY2YzYxNzQ2NTY0NWY2MTY2NzQ2NTcyMjIzQTIyMzIzMDMyCjMwMmQzMDMxMmQzMDMxMjI3RDoyMTI5Yjc3ZTlmOTZjYTRlYWE2YjVmZjVkNTBlNTBhZWM5MTk3Mzg5NWVlYTMKNzcwMTk5NmMzMDQxMDU1YjA1N2VmMzNlMzI1YzUzNzUzOTkzZDE4NmRlYjlmYmU1MTBmOTk2MTQ3NTg3ZGI0MAoxNmM2MzJhODhjOWI0YTBiZjc3ZTdlOTliZmUzNjEyMmVhY2I4YTlhZDA4YWFmMTVjYzQ4Mzg0OTY1OTU5MzUxCjE2MjBjNDMzNzQ0MWJiZjFiODMzM2ZhZGZlMGQzZmU0NDRlNDExMmQyMGVlN2YyNjg3NTBkMzY0ODc0MjZhMTgKM2I5MzE5MTg1YzRkNDdiZDQ0ZWE5YjE3OTEzNDBmNWU2Y2JhMTFlYzBlMGM3NDM3Y2VlN2MyZWNmOGVkZGI5OQo1YTU5NDcwNjAzZjc3MWI5ZGU4ZWRhYzU5OTBkYjEyMTM2Yzg5YTBiZTk2NGFhZjUzOWNjMWI4MDQ1ODE4OTk1CmY2NzJkMTE0MDYyMDY5Njg0ZTFkM2YxYjBhZGI0NzUxZTQwMDgzZmRlMDRmNGUwMWE3MTBlYzIyYzFhMWZjNTUKOTNjN2QyZWQxZWQ2MTVmOWJhOTE3ODJiYTMzNzAwZjI5MWE5YzZkODZmOGJiYTc0YzUwNzlhNmYyZDA0NmM2Nwo2YTVkNTdjYmNmOTA3MWQ4OGRhCi0tLS0tRU5EIE5FVFJPVU5EUyBMSUNFTlNFLS0tLS0K
Error: identified at least one change, exiting with non-zero exit code (detailed-exitcode parameter enabled)
identified at least one change, exiting with non-zero exit code (detailed-exitcode parameter enabled)
Error: plugin "diff" exited with error

********************

	Release was not present in Helm.  Diff will show entire contents as new.

********************
kube-system, external-dns, ServiceAccount (v1) has been added:
- 
+ # Source: external-dns/templates/serviceaccount.yaml
+ apiVersion: v1
+ kind: ServiceAccount
+ metadata:
+   labels:     
+     app: external-dns
+     heritage: Tiller
+     release: external-dns
+     chart: external-dns-1.3.3
+   name: external-dns
kube-system, external-dns, ClusterRole (rbac.authorization.k8s.io) has been added:
- 
+ # Source: external-dns/templates/clusterrole.yaml
+ apiVersion: rbac.authorization.k8s.io/v1beta1
+ kind: ClusterRole
+ metadata:
+   labels:     
+     app: external-dns
+     heritage: Tiller
+     release: external-dns
+     chart: external-dns-1.3.3
+   name: external-dns
+ rules:
+   - apiGroups:
+       - ""
+       - extensions
+       - networking.istio.io
+     resources:
+       - ingresses
+       - services
+       - pods
+       - nodes
+       - gateways
+     verbs:
+       - get
+       - list
+       - watch
kube-system, external-dns, ClusterRoleBinding (rbac.authorization.k8s.io) has been added:
- 
+ # Source: external-dns/templates/clusterrolebinding.yaml
+ apiVersion: rbac.authorization.k8s.io/v1beta1
+ kind: ClusterRoleBinding
+ metadata:
+   labels:     
+     app: external-dns
+     heritage: Tiller
+     release: external-dns
+     chart: external-dns-1.3.3
+   name: external-dns
+ roleRef:
+   apiGroup: rbac.authorization.k8s.io
+   kind: ClusterRole
+   name: external-dns
+ subjects:
+   - kind: ServiceAccount
+     name: external-dns
+     namespace: kube-system
kube-system, external-dns, Service (v1) has been added:
- 
+ # Source: external-dns/templates/service.yaml
+ apiVersion: v1
+ kind: Service
+ metadata:
+   labels:
+     app: external-dns
+     chart: external-dns-1.3.3
+     heritage: Tiller
+     release: external-dns
+   name: external-dns
+ spec:
+   ports:
+     - port: 7979
+       protocol: TCP
+       targetPort: 7979
+       name: http
+   selector:
+     app: external-dns
+     release: external-dns
+   type: "ClusterIP"
kube-system, external-dns, Deployment (extensions) has been added:
- 
+ # Source: external-dns/templates/deployment.yaml
+ apiVersion: extensions/v1beta1
+ kind: Deployment
+ metadata:
+   labels:     
+     app: external-dns
+     heritage: Tiller
+     release: external-dns
+     chart: external-dns-1.3.3
+   name: external-dns
+ spec:
+   template:
+     metadata:
+       labels:         
+         app: external-dns
+         heritage: Tiller
+         release: external-dns
+     spec:
+       containers:
+         - name: external-dns
+           image: "registry.opensource.zalan.do/teapot/external-dns:v0.5.9"
+           imagePullPolicy: "IfNotPresent"
+           args:
+             - --log-level=debug
+             - --domain-filter=test.netrounds.com
+             - --policy=upsert-only
+             - --provider=aws
+             - --registry=txt
+             - --txt-owner-id=netrounds
+             - --txt-prefix=owner_
+             - --source=service
+             - --aws-zone-type=public
+           volumeMounts:
+           env:
+           - name: AWS_DEFAULT_REGION
+             value: eu-north-1
+           livenessProbe:
+             httpGet:
+               path: /healthz
+               port: 7979
+           ports:
+             - containerPort: 7979
+           resources:
+             limits: {}
+             requests:
+               cpu: 10m
+               memory: 20Mi
+             
+       volumes:
+       serviceAccountName: external-dns
Error: identified at least one change, exiting with non-zero exit code (detailed-exitcode parameter enabled)
identified at least one change, exiting with non-zero exit code (detailed-exitcode parameter enabled)
Error: plugin "diff" exited with error

********************

	Release was not present in Helm.  Diff will show entire contents as new.

********************
kube-system, kubernetes-dashboard, Secret (v1) has been added:
- 
+ # Source: kubernetes-dashboard/templates/secret.yaml
+ apiVersion: v1
+ kind: Secret
+ metadata:
+   labels:
+     app: kubernetes-dashboard
+     chart: kubernetes-dashboard-1.0.1
+     heritage: Tiller
+     release: kubernetes-dashboard
+   name: kubernetes-dashboard
+   namespace: kube-system
+ type: Opaque
kube-system, kubernetes-dashboard, ServiceAccount (v1) has been added:
- 
+ # Source: kubernetes-dashboard/templates/serviceaccount.yaml
+ apiVersion: v1
+ kind: ServiceAccount
+ metadata:
+   labels:
+     app: kubernetes-dashboard
+     chart: kubernetes-dashboard-1.0.1
+     heritage: Tiller
+     release: kubernetes-dashboard
+   name: kubernetes-dashboard
+   namespace: kube-system
kube-system, kubernetes-dashboard, ClusterRoleBinding (rbac.authorization.k8s.io) has been added:
- 
+ # Source: kubernetes-dashboard/templates/rolebinding.yaml
+ # Cluster role binding for clusterAdminRole == true
+ apiVersion: rbac.authorization.k8s.io/v1beta1
+ kind: ClusterRoleBinding
+ metadata:
+   labels:
+     app: kubernetes-dashboard
+     chart: kubernetes-dashboard-1.0.1
+     heritage: Tiller
+     release: kubernetes-dashboard
+   name: kubernetes-dashboard
+ roleRef:
+   apiGroup: rbac.authorization.k8s.io
+   kind: ClusterRole
+   name: cluster-admin
+ subjects:
+   - kind: ServiceAccount
+     name: kubernetes-dashboard
+     namespace: kube-system
kube-system, kubernetes-dashboard, Service (v1) has been added:
- 
+ # Source: kubernetes-dashboard/templates/svc.yaml
+ apiVersion: v1
+ kind: Service
+ metadata:
+   name: kubernetes-dashboard
+   labels:
+     app: kubernetes-dashboard
+     chart: kubernetes-dashboard-1.0.1
+     release: kubernetes-dashboard
+     heritage: Tiller
+     kubernetes.io/cluster-service: "true"
+ spec:
+   type: ClusterIP
+   ports:
+   - port: 443
+     targetPort: 8443
+     name: "https"
+   selector:
+     app: kubernetes-dashboard
+     release: kubernetes-dashboard
kube-system, kubernetes-dashboard, Deployment (extensions) has been added:
- 
+ # Source: kubernetes-dashboard/templates/deployment.yaml
+ apiVersion: extensions/v1beta1
+ kind: Deployment
+ metadata:
+   name: kubernetes-dashboard
+   labels:
+     app: kubernetes-dashboard
+     chart: kubernetes-dashboard-1.0.1
+     release: kubernetes-dashboard
+     heritage: Tiller
+     kubernetes.io/cluster-service: "true"
+ spec:
+   replicas: 1
+   strategy:
+     rollingUpdate:
+       maxSurge: 0
+       maxUnavailable: 1
+     type: RollingUpdate
+   template:
+     metadata:
+       labels:
+         app: kubernetes-dashboard
+         release: kubernetes-dashboard
+         kubernetes.io/cluster-service: "true"
+     spec:
+       serviceAccountName: kubernetes-dashboard
+       containers:
+       - name: kubernetes-dashboard
+         image: "k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1"
+         imagePullPolicy: IfNotPresent
+         args:
+           - --auto-generate-certificates
+         ports:
+         - name: https
+           containerPort: 8443
+           protocol: TCP
+         volumeMounts:
+         - name: kubernetes-dashboard-certs
+           mountPath: /certs
+           # Create on-disk volume to store exec logs
+         - mountPath: /tmp
+           name: tmp-volume
+         livenessProbe:
+           httpGet:
+             scheme: HTTPS
+             path: /
+             port: 8443
+           initialDelaySeconds: 30
+           timeoutSeconds: 30
+         resources:
+           limits:
+             cpu: 100m
+             memory: 100Mi
+           requests:
+             cpu: 100m
+             memory: 100Mi
+           
+       volumes:
+       - name: kubernetes-dashboard-certs
+         secret:
+           secretName: kubernetes-dashboard
+       - name: tmp-volume
+         emptyDir: {}
Error: identified at least one change, exiting with non-zero exit code (detailed-exitcode parameter enabled)
identified at least one change, exiting with non-zero exit code (detailed-exitcode parameter enabled)
Error: plugin "diff" exited with error

********************

	Release was not present in Helm.  Diff will show entire contents as new.

********************
kube-system, nginx-ingress-default-backend, Deployment (extensions) has been added:
- 
+ # Source: nginx-ingress/templates/default-backend-deployment.yaml
+ apiVersion: extensions/v1beta1
+ kind: Deployment
+ metadata:
+   labels:
+     app: nginx-ingress
+     chart: nginx-ingress-1.1.5
+     component: "default-backend"
+     heritage: Tiller
+     release: nginx-ingress
+   name: nginx-ingress-default-backend
+ spec:
+   replicas: 1
+   revisionHistoryLimit: 10
+   template:
+     metadata:
+       labels:
+         app: nginx-ingress
+         component: "default-backend"
+         release: nginx-ingress
+     spec:
+       containers:
+         - name: nginx-ingress-default-backend
+           image: "k8s.gcr.io/defaultbackend:1.4"
+           imagePullPolicy: "IfNotPresent"
+           args:
+           livenessProbe:
+             httpGet:
+               path: /healthz
+               port: 8080
+               scheme: HTTP
+             initialDelaySeconds: 30
+             timeoutSeconds: 5
+           ports:
+             - name: http
+               containerPort: 8080
+               protocol: TCP
+           resources:
+             {}
+             
+       terminationGracePeriodSeconds: 60
kube-system, nginx-ingress-tcp, ConfigMap (v1) has been added:
- 
+ # Source: nginx-ingress/templates/tcp-configmap.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   labels:
+     app: nginx-ingress
+     chart: nginx-ingress-1.1.5
+     component: "controller"
+     heritage: Tiller
+     release: nginx-ingress
+   name: nginx-ingress-tcp
+ data:
+   "6000": default/ncc:6000
kube-system, nginx-ingress, Role (rbac.authorization.k8s.io) has been added:
- 
+ # Source: nginx-ingress/templates/role.yaml
+ apiVersion: rbac.authorization.k8s.io/v1beta1
+ kind: Role
+ metadata:
+   labels:
+     app: nginx-ingress
+     chart: nginx-ingress-1.1.5
+     heritage: Tiller
+     release: nginx-ingress
+   name: nginx-ingress
+ rules:
+   - apiGroups:
+       - ""
+     resources:
+       - namespaces
+     verbs:
+       - get
+   - apiGroups:
+       - ""
+     resources:
+       - configmaps
+       - pods
+       - secrets
+       - endpoints
+     verbs:
+       - get
+       - list
+       - watch
+   - apiGroups:
+       - ""
+     resources:
+       - services
+     verbs:
+       - get
+       - list
+       - update
+       - watch
+   - apiGroups:
+       - extensions
+     resources:
+       - ingresses
+     verbs:
+       - get
+       - list
+       - watch
+   - apiGroups:
+       - extensions
+     resources:
+       - ingresses/status
+     verbs:
+       - update
+   - apiGroups:
+       - ""
+     resources:
+       - configmaps
+     resourceNames:
+       - ingress-controller-leader-nginx
+     verbs:
+       - get
+       - update
+   - apiGroups:
+       - ""
+     resources:
+       - configmaps
+     verbs:
+       - create
+   - apiGroups:
+       - ""
+     resources:
+       - endpoints
+     verbs:
+       - create
+       - get
+       - update
+   - apiGroups:
+       - ""
+     resources:
+       - events
+     verbs:
+       - create
+       - patch
kube-system, nginx-ingress-controller, Service (v1) has been added:
- 
+ # Source: nginx-ingress/templates/controller-service.yaml
+ apiVersion: v1
+ kind: Service
+ metadata:
+   annotations:
+     external-dns.alpha.kubernetes.io/hostname: "app.netrounds-niclas-19-eks.test.netrounds.com,agent.netrounds-niclas-19-eks.test.netrounds.com"
+   labels:
+     app: nginx-ingress
+     chart: nginx-ingress-1.1.5
+     component: "controller"
+     heritage: Tiller
+     release: nginx-ingress
+   name: nginx-ingress-controller
+ spec:
+   ports:
+     - name: http
+       port: 80
+       protocol: TCP
+       targetPort: http
+     - name: https
+       port: 443
+       protocol: TCP
+       targetPort: https
+     - name: "6000-tcp"
+       port: 6000
+       protocol: TCP
+       targetPort: "6000-tcp"
+   selector:
+     app: nginx-ingress
+     component: "controller"
+     release: nginx-ingress
+   type: "LoadBalancer"
kube-system, nginx-ingress, ClusterRoleBinding (rbac.authorization.k8s.io) has been added:
- 
+ # Source: nginx-ingress/templates/clusterrolebinding.yaml
+ apiVersion: rbac.authorization.k8s.io/v1beta1
+ kind: ClusterRoleBinding
+ metadata:
+   labels:
+     app: nginx-ingress
+     chart: nginx-ingress-1.1.5
+     heritage: Tiller
+     release: nginx-ingress
+   name: nginx-ingress
+ roleRef:
+   apiGroup: rbac.authorization.k8s.io
+   kind: ClusterRole
+   name: nginx-ingress
+ subjects:
+   - kind: ServiceAccount
+     name: nginx-ingress
+     namespace: kube-system
kube-system, nginx-ingress, RoleBinding (rbac.authorization.k8s.io) has been added:
- 
+ # Source: nginx-ingress/templates/rolebinding.yaml
+ apiVersion: rbac.authorization.k8s.io/v1beta1
+ kind: RoleBinding
+ metadata:
+   labels:
+     app: nginx-ingress
+     chart: nginx-ingress-1.1.5
+     heritage: Tiller
+     release: nginx-ingress
+   name: nginx-ingress
+ roleRef:
+   apiGroup: rbac.authorization.k8s.io
+   kind: Role
+   name: nginx-ingress
+ subjects:
+   - kind: ServiceAccount
+     name: nginx-ingress
+     namespace: kube-system
kube-system, nginx-ingress-default-backend, Service (v1) has been added:
- 
+ # Source: nginx-ingress/templates/default-backend-service.yaml
+ apiVersion: v1
+ kind: Service
+ metadata:
+   labels:
+     app: nginx-ingress
+     chart: nginx-ingress-1.1.5
+     component: "default-backend"
+     heritage: Tiller
+     release: nginx-ingress
+   name: nginx-ingress-default-backend
+ spec:
+   clusterIP: ""
+   ports:
+     - name: http
+       port: 80
+       protocol: TCP
+       targetPort: http
+   selector:
+     app: nginx-ingress
+     component: "default-backend"
+     release: nginx-ingress
+   type: "ClusterIP"
kube-system, nginx-ingress-controller, Deployment (extensions) has been added:
- 
+ # Source: nginx-ingress/templates/controller-deployment.yaml
+ apiVersion: extensions/v1beta1
+ kind: Deployment
+ metadata:
+   labels:
+     app: nginx-ingress
+     chart: nginx-ingress-1.1.5
+     component: "controller"
+     heritage: Tiller
+     release: nginx-ingress
+   name: nginx-ingress-controller
+ spec:
+   replicas: 1
+   revisionHistoryLimit: 10
+   strategy:
+     {}
+     
+   minReadySeconds: 0
+   template:
+     metadata:
+       labels:
+         app: nginx-ingress
+         component: "controller"
+         release: nginx-ingress
+     spec:
+       dnsPolicy: ClusterFirst
+       containers:
+         - name: nginx-ingress-controller
+           image: "quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.22.0"
+           imagePullPolicy: "IfNotPresent"
+           args:
+             - /nginx-ingress-controller
+             - --default-backend-service=kube-system/nginx-ingress-default-backend
+             - --election-id=ingress-controller-leader
+             - --ingress-class=nginx
+             - --configmap=kube-system/nginx-ingress-controller
+             - --tcp-services-configmap=kube-system/nginx-ingress-tcp
+           securityContext:
+             capabilities:
+                 drop:
+                 - ALL
+                 add:
+                 - NET_BIND_SERVICE
+             runAsUser: 33
+           env:
+             - name: POD_NAME
+               valueFrom:
+                 fieldRef:
+                   fieldPath: metadata.name
+             - name: POD_NAMESPACE
+               valueFrom:
+                 fieldRef:
+                   fieldPath: metadata.namespace
+           livenessProbe:
+             httpGet:
+               path: /healthz
+               port: 10254
+               scheme: HTTP
+             initialDelaySeconds: 10
+             periodSeconds: 10
+             timeoutSeconds: 1
+             successThreshold: 1
+             failureThreshold: 3
+           ports:
+             - name: http
+               containerPort: 80
+               protocol: TCP
+             - name: https
+               containerPort: 443
+               protocol: TCP
+             - name: "6000-tcp"
+               containerPort: 6000
+               protocol: TCP
+           readinessProbe:
+             httpGet:
+               path: /healthz
+               port: 10254
+               scheme: HTTP
+             initialDelaySeconds: 10
+             periodSeconds: 10
+             timeoutSeconds: 1
+             successThreshold: 1
+             failureThreshold: 3
+           resources:
+             {}
+             
+       hostNetwork: false
+       serviceAccountName: nginx-ingress
+       terminationGracePeriodSeconds: 60
kube-system, nginx-ingress-controller, ConfigMap (v1) has been added:
- 
+ # Source: nginx-ingress/templates/controller-configmap.yaml
+ apiVersion: v1
+ kind: ConfigMap
+ metadata:
+   labels:
+     app: nginx-ingress
+     chart: nginx-ingress-1.1.5
+     component: "controller"
+     heritage: Tiller
+     release: nginx-ingress
+   name: nginx-ingress-controller
+ data:
+   enable-vts-status: "false"
kube-system, nginx-ingress, ServiceAccount (v1) has been added:
- 
+ # Source: nginx-ingress/templates/serviceaccount.yaml
+ apiVersion: v1
+ kind: ServiceAccount
+ metadata:
+   labels:
+     app: nginx-ingress
+     chart: nginx-ingress-1.1.5
+     heritage: Tiller
+     release: nginx-ingress
+   name: nginx-ingress
kube-system, nginx-ingress, ClusterRole (rbac.authorization.k8s.io) has been added:
- 
+ # Source: nginx-ingress/templates/clusterrole.yaml
+ apiVersion: rbac.authorization.k8s.io/v1beta1
+ kind: ClusterRole
+ metadata:
+   labels:
+     app: nginx-ingress
+     chart: nginx-ingress-1.1.5
+     heritage: Tiller
+     release: nginx-ingress
+   name: nginx-ingress
+ rules:
+   - apiGroups:
+       - ""
+     resources:
+       - configmaps
+       - endpoints
+       - nodes
+       - pods
+       - secrets
+     verbs:
+       - list
+       - watch
+   - apiGroups:
+       - ""
+     resources:
+       - nodes
+     verbs:
+       - get
+   - apiGroups:
+       - ""
+     resources:
+       - services
+     verbs:
+       - get
+       - list
+       - update
+       - watch
+   - apiGroups:
+       - extensions
+     resources:
+       - ingresses
+     verbs:
+       - get
+       - list
+       - watch
+   - apiGroups:
+       - ""
+     resources:
+       - events
+     verbs:
+       - create
+       - patch
+   - apiGroups:
+       - extensions
+     resources:
+       - ingresses/status
+     verbs:
+       - update
Error: identified at least one change, exiting with non-zero exit code (detailed-exitcode parameter enabled)
identified at least one change, exiting with non-zero exit code (detailed-exitcode parameter enabled)
Error: plugin "diff" exited with error

Upgrading /home/niclas/work/tmp1/tmp/tmp/ncc3/kubernetes/manifests/ncc-objects
Upgrading stable/nginx-ingress
Upgrading stable/external-dns
Upgrading /home/niclas/work/tmp1/tmp/tmp/ncc3/kubernetes/manifests/ncc-full
Upgrading stable/kubernetes-dashboard
Release "ncc-objects" does not exist. Installing it now.
NAME:   ncc-objects
LAST DEPLOYED: Wed Mar 13 06:10:33 2019
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==> v1/LimitRange
NAME                AGE
ncc-objects-limits  3s

==> v1/PersistentVolumeClaim
NAME                       STATUS  VOLUME                                    CAPACITY  ACCESS MODES  STORAGECLASS  AGE
ncc-objects-mysql-storage  Bound   pvc-5462e4e1-454e-11e9-9172-06ad1f6e4ae4  100Gi     RWO           aws-gp2       3s
ncc-objects-ncc-storage    Bound   pvc-54637b47-454e-11e9-9172-06ad1f6e4ae4  100Gi     RWO           aws-gp2       3s

==> v1/Secret
NAME              TYPE               DATA  AGE
agent-tls-secret  kubernetes.io/tls  2     3s
ncc-tls-secret    kubernetes.io/tls  2     3s

==> v1/StorageClass
NAME       PROVISIONER            AGE
aws-gp2    kubernetes.io/aws-ebs  3s
aws-mysql  kubernetes.io/aws-ebs  3s
aws-ncc    kubernetes.io/aws-ebs  3s



Release "kubernetes-dashboard" does not exist. Installing it now.
NAME:   kubernetes-dashboard
E0313 06:10:38.112745   26184 portforward.go:363] error copying from remote stream to local connection: readfrom tcp4 127.0.0.1:42667->127.0.0.1:60748: write tcp4 127.0.0.1:42667->127.0.0.1:60748: write: broken pipe
LAST DEPLOYED: Wed Mar 13 06:10:35 2019
NAMESPACE: kube-system
STATUS: DEPLOYED

RESOURCES:
==> v1/Pod(related)
NAME                                   READY  STATUS             RESTARTS  AGE
kubernetes-dashboard-5478c45897-z89mv  0/1    ContainerCreating  0         3s

==> v1/Secret
NAME                  TYPE    DATA  AGE
kubernetes-dashboard  Opaque  0     3s

==> v1/Service
NAME                  TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)  AGE
kubernetes-dashboard  ClusterIP  172.20.223.217  <none>       443/TCP  3s

==> v1/ServiceAccount
NAME                  SECRETS  AGE
kubernetes-dashboard  1        3s

==> v1beta1/ClusterRoleBinding
NAME                  AGE
kubernetes-dashboard  3s

==> v1beta1/Deployment
NAME                  READY  UP-TO-DATE  AVAILABLE  AGE
kubernetes-dashboard  0/1    1           0          3s


NOTES:
*********************************************************************************
*** PLEASE BE PATIENT: kubernetes-dashboard may take a few minutes to install ***
*********************************************************************************

Get the Kubernetes Dashboard URL by running:
  export POD_NAME=$(kubectl get pods -n kube-system -l "app=kubernetes-dashboard,release=kubernetes-dashboard" -o jsonpath="{.items[0].metadata.name}")
  echo https://127.0.0.1:8443/
  kubectl -n kube-system port-forward $POD_NAME 8443:8443


Release "external-dns" does not exist. Installing it now.
NAME:   external-dns
E0313 06:10:38.419013   26195 portforward.go:363] error copying from remote stream to local connection: readfrom tcp4 127.0.0.1:42837->127.0.0.1:56284: write tcp4 127.0.0.1:42837->127.0.0.1:56284: write: broken pipe
LAST DEPLOYED: Wed Mar 13 06:10:35 2019
NAMESPACE: kube-system
STATUS: DEPLOYED

RESOURCES:
==> v1/Pod(related)
NAME                           READY  STATUS             RESTARTS  AGE
external-dns-554c5d8dfd-h7m49  0/1    ContainerCreating  0         2s

==> v1/Service
NAME          TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)   AGE
external-dns  ClusterIP  172.20.69.122  <none>       7979/TCP  2s

==> v1/ServiceAccount
NAME          SECRETS  AGE
external-dns  1        2s

==> v1beta1/ClusterRole
NAME          AGE
external-dns  2s

==> v1beta1/ClusterRoleBinding
NAME          AGE
external-dns  2s

==> v1beta1/Deployment
NAME          READY  UP-TO-DATE  AVAILABLE  AGE
external-dns  0/1    1           0          2s


NOTES:
To verify that external-dns has started, run:

  kubectl --namespace=kube-system get pods -l "app=external-dns,release=external-dns"


Release "nginx-ingress" does not exist. Installing it now.
NAME:   nginx-ingress
LAST DEPLOYED: Wed Mar 13 06:10:35 2019
NAMESPACE: kube-system
STATUS: DEPLOYED

RESOURCES:
==> v1/ConfigMap
NAME                      DATA  AGE
nginx-ingress-controller  1     5s
nginx-ingress-tcp         1     5s

==> v1/Pod(related)
NAME                                            READY  STATUS             RESTARTS  AGE
nginx-ingress-controller-854dc847fb-g9nsf       0/1    ContainerCreating  0         5s
nginx-ingress-default-backend-544cfb69fc-fpgc6  1/1    Running            0         5s

==> v1/Service
NAME                           TYPE          CLUSTER-IP      EXTERNAL-IP       PORT(S)                                    AGE
nginx-ingress-controller       LoadBalancer  172.20.209.141  a55ada5c5454e...  80:31685/TCP,443:32569/TCP,6000:31694/TCP  5s
nginx-ingress-default-backend  ClusterIP     172.20.20.45    <none>            80/TCP                                     5s

==> v1/ServiceAccount
NAME           SECRETS  AGE
nginx-ingress  1        5s

==> v1beta1/ClusterRole
NAME           AGE
nginx-ingress  5s

==> v1beta1/ClusterRoleBinding
NAME           AGE
nginx-ingress  5s

==> v1beta1/Deployment
NAME                           READY  UP-TO-DATE  AVAILABLE  AGE
nginx-ingress-controller       0/1    1           0          5s
nginx-ingress-default-backend  1/1    1           1          5s

==> v1beta1/Role
NAME           AGE
nginx-ingress  5s

==> v1beta1/RoleBinding
NAME           AGE
nginx-ingress  5s


NOTES:
The nginx-ingress controller has been installed.
It may take a few minutes for the LoadBalancer IP to be available.
You can watch the status by running 'kubectl --namespace kube-system get services -o wide -w nginx-ingress-controller'

An example Ingress that makes use of the controller:

  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      kubernetes.io/ingress.class: nginx
    name: example
    namespace: foo
  spec:
    rules:
      - host: www.example.com
        http:
          paths:
            - backend:
                serviceName: exampleService
                servicePort: 80
              path: /
    # This section is only required if TLS is to be enabled for the Ingress
    tls:
        - hosts:
            - www.example.com
          secretName: example-tls

If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:

  apiVersion: v1
  kind: Secret
  metadata:
    name: example-tls
    namespace: foo
  data:
    tls.crt: <base64 encoded cert>
    tls.key: <base64 encoded key>
  type: kubernetes.io/tls


Release "ncc-full" does not exist. Installing it now.
2019/03/13 06:10:33 Warning: Building values map for chart 'restol'. Skipped value (map[]) for 'deployment', as it is not a table.
2019/03/13 06:10:33 Warning: Building values map for chart 'restol'. Skipped value (map[]) for 'deployment', as it is not a table.
2019/03/13 06:10:33 Warning: Building values map for chart 'restol'. Skipped value (map[]) for 'deployment', as it is not a table.
2019/03/13 06:10:33 Warning: Building values map for chart 'restol'. Skipped value (map[]) for 'deployment', as it is not a table.
Error: release ncc-full failed: timed out waiting for the condition

err: release "ncc-full" in "helmfile.yaml" failed: exit status 1
exit status 1
2019-03-13T06:40:36+01:00 FTL failed to run install on software provider: failed to run: helmfile --file /home/niclas/work/tmp1/tmp/tmp/ncc3/env/helmfile.yaml --environment eks apply --args '--tls --tls-ca-cert=/home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state/ca.pem --tls-cert=/home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state/helm.pem --tls-key=/home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state/helm-key.pem': command exited with:   exit status 1
  helmfile --file /home/niclas/work/tmp1/tmp/tmp/ncc3/env/helmfile.yaml --environment eks apply --args '--tls --tls-ca-cert=/home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state/ca.pem --tls-cert=/home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state/helm.pem --tls-key=/home/niclas/work/tmp1/tmp/tmp/ncc3/eks-state/helm-key.pem'
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ ls
bin        eks-values.sh  _istio               kubernetes    README.md      services
docs       env            Jenkinsfile          netroundsctl  README-WIN.md
eks-state  istio          Jenkinsfile.staging  packaging     scripts
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ source <(netroundsctl cluster kubeenv)
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ source <(kubectl completion bash)
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ kubectl get pods
No resources found.
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ kubectl get pods -n istio-system
NAME                                      READY   STATUS    RESTARTS   AGE
istio-citadel-796c94878b-wwtmf            1/1     Running   0          36m
istio-egressgateway-864444d6ff-n6bs4      1/1     Running   0          36m
istio-galley-6c68c5dbcf-wn7wm             1/1     Running   0          36m
istio-ingressgateway-694576c7bb-kkxbl     1/1     Running   0          36m
istio-pilot-79f5f46dd5-8hq2z              2/2     Running   0          36m
istio-policy-5bd5578b94-zvkz8             2/2     Running   0          36m
istio-sidecar-injector-6d8f88c98f-48xrt   1/1     Running   0          36m
istio-telemetry-5598f86cd8-6r98f          2/2     Running   0          36m
prometheus-76db5fddd5-6bnbc               1/1     Running   0          36m
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ kubectl get pods -n kube-system
NAME                                             READY   STATUS    RESTARTS   AGE
aws-node-8bhgm                                   1/1     Running   0          37m
coredns-57dfc4f694-lt5f9                         1/1     Running   0          39m
coredns-57dfc4f694-rdw9g                         1/1     Running   0          39m
external-dns-554c5d8dfd-h7m49                    1/1     Running   0          36m
kube-proxy-vr5dr                                 1/1     Running   0          37m
kubernetes-dashboard-5478c45897-z89mv            1/1     Running   0          36m
nginx-ingress-controller-854dc847fb-g9nsf        1/1     Running   0          36m
nginx-ingress-default-backend-544cfb69fc-fpgc6   1/1     Running   0          36m
registry-creds-6f9d588b79-4vppg                  1/1     Running   0          36m
tiller-deploy-f4d44bd5f-7sk49                    1/1     Running   0          37m
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ ls
bin        eks-values.sh  _istio               kubernetes    README.md      services
docs       env            Jenkinsfile          netroundsctl  README-WIN.md
eks-state  istio          Jenkinsfile.staging  packaging     scripts
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ kubectl get pods
No resources found.
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ kubectl get logs
error: the server doesn't have a resource type "logs"
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ kubectl get events
LAST SEEN   FIRST SEEN   COUNT   NAME                                                          KIND                    SUBOBJECT   TYPE      REASON                    SOURCE                                                   MESSAGE
38m         38m          2       ip-10-35-0-219.eu-north-1.compute.internal.158b6c7533d7af2e   Node                                Normal    NodeHasSufficientDisk     kubelet, ip-10-35-0-219.eu-north-1.compute.internal      Node ip-10-35-0-219.eu-north-1.compute.internal status is now: NodeHasSufficientDisk
38m         38m          1       ip-10-35-0-219.eu-north-1.compute.internal.158b6c7534fa5085   Node                                Normal    NodeAllocatableEnforced   kubelet, ip-10-35-0-219.eu-north-1.compute.internal      Updated Node Allocatable limit across pods
38m         38m          2       ip-10-35-0-219.eu-north-1.compute.internal.158b6c7533d80616   Node                                Normal    NodeHasSufficientPID      kubelet, ip-10-35-0-219.eu-north-1.compute.internal      Node ip-10-35-0-219.eu-north-1.compute.internal status is now: NodeHasSufficientPID
38m         38m          2       ip-10-35-0-219.eu-north-1.compute.internal.158b6c7533d7f6b3   Node                                Normal    NodeHasNoDiskPressure     kubelet, ip-10-35-0-219.eu-north-1.compute.internal      Node ip-10-35-0-219.eu-north-1.compute.internal status is now: NodeHasNoDiskPressure
38m         38m          2       ip-10-35-0-219.eu-north-1.compute.internal.158b6c7533d7e2a7   Node                                Normal    NodeHasSufficientMemory   kubelet, ip-10-35-0-219.eu-north-1.compute.internal      Node ip-10-35-0-219.eu-north-1.compute.internal status is now: NodeHasSufficientMemory
38m         38m          1       ip-10-35-0-219.eu-north-1.compute.internal.158b6c752d60ef04   Node                                Normal    Starting                  kubelet, ip-10-35-0-219.eu-north-1.compute.internal      Starting kubelet.
38m         38m          1       ip-10-35-0-219.eu-north-1.compute.internal.158b6c76b5a2e6f1   Node                                Normal    NodeAllocatableEnforced   kubelet, ip-10-35-0-219.eu-north-1.compute.internal      Updated Node Allocatable limit across pods
38m         38m          1       ip-10-35-0-219.eu-north-1.compute.internal.158b6c76af67b4d8   Node                                Normal    Starting                  kubelet, ip-10-35-0-219.eu-north-1.compute.internal      Starting kubelet.
38m         38m          1       ip-10-35-0-219.eu-north-1.compute.internal.158b6c7855b5462d   Node                                Normal    Starting                  kube-proxy, ip-10-35-0-219.eu-north-1.compute.internal   Starting kube-proxy.
38m         38m          2       ip-10-35-0-219.eu-north-1.compute.internal.158b6c76b59749c1   Node                                Normal    NodeHasSufficientMemory   kubelet, ip-10-35-0-219.eu-north-1.compute.internal      Node ip-10-35-0-219.eu-north-1.compute.internal status is now: NodeHasSufficientMemory
38m         38m          2       ip-10-35-0-219.eu-north-1.compute.internal.158b6c76b5975cc4   Node                                Normal    NodeHasNoDiskPressure     kubelet, ip-10-35-0-219.eu-north-1.compute.internal      Node ip-10-35-0-219.eu-north-1.compute.internal status is now: NodeHasNoDiskPressure
38m         38m          2       ip-10-35-0-219.eu-north-1.compute.internal.158b6c76b5976c6a   Node                                Normal    NodeHasSufficientPID      kubelet, ip-10-35-0-219.eu-north-1.compute.internal      Node ip-10-35-0-219.eu-north-1.compute.internal status is now: NodeHasSufficientPID
38m         38m          2       ip-10-35-0-219.eu-north-1.compute.internal.158b6c76b5971f83   Node                                Normal    NodeHasSufficientDisk     kubelet, ip-10-35-0-219.eu-north-1.compute.internal      Node ip-10-35-0-219.eu-north-1.compute.internal status is now: NodeHasSufficientDisk
38m         38m          1       ip-10-35-0-219.eu-north-1.compute.internal.158b6c790ad62f69   Node                                Normal    NodeReady                 kubelet, ip-10-35-0-219.eu-north-1.compute.internal      Node ip-10-35-0-219.eu-north-1.compute.internal status is now: NodeReady
37m         37m          1       ncc.158b6c897b255f7d                                          Deployment                          Normal    ScalingReplicaSet         deployment-controller                                    Scaled up replica set ncc-7db7b8db7 to 1
37m         37m          1       ncc-objects-mysql-storage.158b6c897a4dc2aa                    PersistentVolumeClaim               Normal    ProvisioningSucceeded     persistentvolume-controller                              Successfully provisioned volume pvc-5462e4e1-454e-11e9-9172-06ad1f6e4ae4 using kubernetes.io/aws-ebs
37m         37m          1       agent-ws-server.158b6c8979bc87f9                              Deployment                          Normal    ScalingReplicaSet         deployment-controller                                    Scaled up replica set agent-ws-server-64b9799765 to 1
37m         37m          1       restol.158b6c897c34285f                                       Deployment                          Normal    ScalingReplicaSet         deployment-controller                                    Scaled up replica set restol-5b879b5b94 to 1
37m         37m          1       ncc-objects-ncc-storage.158b6c897c51bc7d                      PersistentVolumeClaim               Normal    ProvisioningSucceeded     persistentvolume-controller                              Successfully provisioned volume pvc-54637b47-454e-11e9-9172-06ad1f6e4ae4 using kubernetes.io/aws-ebs
37m         37m          1       mysql.158b6c897a34470d                                        Deployment                          Normal    ScalingReplicaSet         deployment-controller                                    Scaled up replica set mysql-665c6d8d95 to 1
37m         37m          1       ncc-openvpn.158b6c8978d1e90e                                  Service                             Normal    EnsuringLoadBalancer      service-controller                                       Ensuring load balancer
37m         37m          1       ncc-openvpn.158b6c89f14c3227                                  Service                             Normal    EnsuredLoadBalancer       service-controller                                       Ensured load balancer
36m         36m          1       ncc.158b6c8f96e6665e                                          Ingress                             Normal    UPDATE                    nginx-ingress-controller                                 Ingress default/ncc
36m         36m          1       ncc.158b6c8f8f2e1f75                                          Ingress                             Normal    CREATE                    nginx-ingress-controller                                 Ingress default/ncc
36m         36m          1       agent-ws-server.158b6c8f96b12e5d                              Ingress                             Normal    UPDATE                    nginx-ingress-controller                                 Ingress default/agent-ws-server
36m         36m          1       agent-ws-server.158b6c8f8f3eafdd                              Ingress                             Normal    CREATE                    nginx-ingress-controller                                 Ingress default/agent-ws-server
36m         36m          1       restol.158b6c8f8f382b86                                       Ingress                             Normal    CREATE                    nginx-ingress-controller                                 Ingress default/restol
36m         36m          1       restol.158b6c8f96da9e79                                       Ingress                             Normal    UPDATE                    nginx-ingress-controller                                 Ingress default/restol
34m         34m          1       agent-ws-server-64b9799765.158b6cb363e7e168                   ReplicaSet                          Warning   FailedCreate              replicaset-controller                                    Error creating: Internal error occurred: failed calling admission webhook "sidecar-injector.istio.io": Post https://istio-sidecar-injector.istio-system.svc:443/inject?timeout=30s: context deadline exceeded
25m         25m          1       ncc-7db7b8db7.158b6d28efe5ac90                                ReplicaSet                          Warning   FailedCreate              replicaset-controller                                    Error creating: Internal error occurred: failed calling admission webhook "sidecar-injector.istio.io": Post https://istio-sidecar-injector.istio-system.svc:443/inject?timeout=30s: dial tcp 10.35.0.223:443: i/o timeout
19m         36m          15      agent-ws-server-64b9799765.158b6c907626ac85                   ReplicaSet                          Warning   FailedCreate              replicaset-controller                                    Error creating: Internal error occurred: failed calling admission webhook "sidecar-injector.istio.io": Post https://istio-sidecar-injector.istio-system.svc:443/inject?timeout=30s: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
19m         36m          17      restol-5b879b5b94.158b6c907895c848                            ReplicaSet                          Warning   FailedCreate              replicaset-controller                                    Error creating: Internal error occurred: failed calling admission webhook "sidecar-injector.istio.io": Post https://istio-sidecar-injector.istio-system.svc:443/inject?timeout=30s: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
8m          36m          19      mysql-665c6d8d95.158b6c907685f75e                             ReplicaSet                          Warning   FailedCreate              replicaset-controller                                    Error creating: Internal error occurred: failed calling admission webhook "sidecar-injector.istio.io": Post https://istio-sidecar-injector.istio-system.svc:443/inject?timeout=30s: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
8m          25m          2       restol-5b879b5b94.158b6d28f0a733f0                            ReplicaSet                          Warning   FailedCreate              replicaset-controller                                    Error creating: Internal error occurred: failed calling admission webhook "sidecar-injector.istio.io": Post https://istio-sidecar-injector.istio-system.svc:443/inject?timeout=30s: dial tcp 10.35.0.223:443: i/o timeout
8m          35m          3       agent-ws-server-64b9799765.158b6c9e6f37935c                   ReplicaSet                          Warning   FailedCreate              replicaset-controller                                    Error creating: Internal error occurred: failed calling admission webhook "sidecar-injector.istio.io": Post https://istio-sidecar-injector.istio-system.svc:443/inject?timeout=30s: dial tcp 10.35.0.223:443: i/o timeout
8m          36m          18      ncc-7db7b8db7.158b6c9077aafd92                                ReplicaSet                          Warning   FailedCreate              replicaset-controller                                    Error creating: Internal error occurred: failed calling admission webhook "sidecar-injector.istio.io": Post https://istio-sidecar-injector.istio-system.svc:443/inject?timeout=30s: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ kubectl describe pod ncc-7db7b8db7.158b6c9077aafd92 
Error from server (NotFound): pods "ncc-7db7b8db7.158b6c9077aafd92" not found
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ kubectl describe po ncc-7db7b8db7.158b6c9077aafd92 
Error from server (NotFound): pods "ncc-7db7b8db7.158b6c9077aafd92" not found
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ kubectl describe pod ncc-7db7b8db7.158b6c9077aafd92 


Error from server (NotFound): pods "ncc-7db7b8db7.158b6c9077aafd92" not found
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ 
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ 
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ kubectl proxy &
[2] 28171
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ curl -s localhost:8001/metrics | grep sidecar-injectorStarting to serve on 127.0.0.1:8001

niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ curl -s localhost:8001/metrics | grep sidecar-injector
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ curl -s localhost:8001/logs/kube-apiserver.log | grep sidecar-injector
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ pod=$(kubectl -n istio-system get pod -listio=sidecar-injector -o jsonpath='{.items[0].metadata.name}')
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ kubectl -n istio-system logs ${pod}
2019-03-13T05:10:09.541859Z	info	version root@464fc845-2bf8-11e9-b805-0a580a2c0506-docker.io/istio-1.0.6-98598f88f6ee9c1e6b3f03b652d8e0e3cd114fa2-dirty-Modified
2019-03-13T05:10:09.542764Z	info	New configuration: sha256sum adfa6417eee148f85fe9a0fa597410f333f44c007ef648c74373b6d50e1efc5a
2019-03-13T05:10:09.542780Z	info	Policy: enabled
2019-03-13T05:10:09.542795Z	info	Template: |
  initContainers:
  - name: istio-init
    image: "docker.io/istio/proxy_init:1.0.6"
    args:
    - "-p"
    - [[ .MeshConfig.ProxyListenPort ]]
    - "-u"
    - 1337
    - "-m"
    - [[ annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode ]]
    - "-i"
    - "[[ annotation .ObjectMeta `traffic.sidecar.istio.io/includeOutboundIPRanges`  "*"  ]]"
    - "-x"
    - "[[ annotation .ObjectMeta `traffic.sidecar.istio.io/excludeOutboundIPRanges`  ""  ]]"
    - "-b"
    - "[[ annotation .ObjectMeta `traffic.sidecar.istio.io/includeInboundPorts` (includeInboundPorts .Spec.Containers) ]]"
    - "-d"
    - "[[ excludeInboundPort (annotation .ObjectMeta `status.sidecar.istio.io/port`  0 ) (annotation .ObjectMeta `traffic.sidecar.istio.io/excludeInboundPorts`  "" ) ]]"
    imagePullPolicy: IfNotPresent
    securityContext:
      capabilities:
        add:
        - NET_ADMIN
      privileged: true
    restartPolicy: Always
  containers:
  - name: istio-proxy
    image: [[ annotation .ObjectMeta `sidecar.istio.io/proxyImage`  "docker.io/istio/proxyv2:1.0.6"  ]]
  
    ports:
    - containerPort: 15090
      protocol: TCP
      name: http-envoy-prom
  
    args:
    - proxy
    - sidecar
    - --configPath
    - [[ .ProxyConfig.ConfigPath ]]
    - --binaryPath
    - [[ .ProxyConfig.BinaryPath ]]
    - --serviceCluster
    [[ if ne "" (index .ObjectMeta.Labels "app") -]]
    - [[ index .ObjectMeta.Labels "app" ]]
    [[ else -]]
    - "istio-proxy"
    [[ end -]]
    - --drainDuration
    - [[ formatDuration .ProxyConfig.DrainDuration ]]
    - --parentShutdownDuration
    - [[ formatDuration .ProxyConfig.ParentShutdownDuration ]]
    - --discoveryAddress
    - [[ annotation .ObjectMeta `sidecar.istio.io/discoveryAddress` .ProxyConfig.DiscoveryAddress ]]
    - --discoveryRefreshDelay
    - [[ formatDuration .ProxyConfig.DiscoveryRefreshDelay ]]
    - --zipkinAddress
    - [[ .ProxyConfig.ZipkinAddress ]]
    - --connectTimeout
    - [[ formatDuration .ProxyConfig.ConnectTimeout ]]
    - --proxyAdminPort
    - [[ .ProxyConfig.ProxyAdminPort ]]
    [[ if gt .ProxyConfig.Concurrency 0 -]]
    - --concurrency
    - [[ .ProxyConfig.Concurrency ]]
    [[ end -]]
    - --controlPlaneAuthPolicy
    - [[ annotation .ObjectMeta `sidecar.istio.io/controlPlaneAuthPolicy` .ProxyConfig.ControlPlaneAuthPolicy ]]
  [[- if (ne (annotation .ObjectMeta `status.sidecar.istio.io/port`  0 ) "0") ]]
    - --statusPort
    - [[ annotation .ObjectMeta `status.sidecar.istio.io/port`  0  ]]
    - --applicationPorts
    - "[[ annotation .ObjectMeta `readiness.status.sidecar.istio.io/applicationPorts` (applicationPorts .Spec.Containers) ]]"
  [[- end ]]
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: INSTANCE_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    - name: ISTIO_META_POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: ISTIO_META_INTERCEPTION_MODE
      value: [[ or (index .ObjectMeta.Annotations "sidecar.istio.io/interceptionMode") .ProxyConfig.InterceptionMode.String ]]
    [[ if .ObjectMeta.Annotations ]]
    - name: ISTIO_METAJSON_ANNOTATIONS
      value: |
             [[ toJson .ObjectMeta.Annotations ]]
    [[ end ]]
    [[ if .ObjectMeta.Labels ]]
    - name: ISTIO_METAJSON_LABELS
      value: |
             [[ toJson .ObjectMeta.Labels ]]
    [[ end ]]
    imagePullPolicy: IfNotPresent
    [[ if (ne (annotation .ObjectMeta `status.sidecar.istio.io/port`  0 ) "0") ]]
    readinessProbe:
      httpGet:
        path: /healthz/ready
        port: [[ annotation .ObjectMeta `status.sidecar.istio.io/port`  0  ]]
      initialDelaySeconds: [[ annotation .ObjectMeta `readiness.status.sidecar.istio.io/initialDelaySeconds`  1  ]]
      periodSeconds: [[ annotation .ObjectMeta `readiness.status.sidecar.istio.io/periodSeconds`  2  ]]
      failureThreshold: [[ annotation .ObjectMeta `readiness.status.sidecar.istio.io/failureThreshold`  30  ]]
    [[ end -]]securityContext:
      
      readOnlyRootFilesystem: true
      [[ if eq (annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode) "TPROXY" -]]
      capabilities:
        add:
        - NET_ADMIN
      runAsGroup: 1337
      [[ else -]]
      runAsUser: 1337
      [[ end -]]
    restartPolicy: Always
    resources:
      [[ if (isset .ObjectMeta.Annotations `sidecar.istio.io/proxyCPU`) -]]
      requests:
        cpu: "[[ index .ObjectMeta.Annotations `sidecar.istio.io/proxyCPU` ]]"
        memory: "[[ index .ObjectMeta.Annotations `sidecar.istio.io/proxyMemory` ]]"
    [[ else -]]
      requests:
        cpu: 10m
      
    [[ end -]]
    volumeMounts:
    - mountPath: /etc/istio/proxy
      name: istio-envoy
    - mountPath: /etc/certs/
      name: istio-certs
      readOnly: true
  volumes:
  - emptyDir:
      medium: Memory
    name: istio-envoy
  - name: istio-certs
    secret:
      optional: true
      [[ if eq .Spec.ServiceAccountName "" -]]
      secretName: istio.default
      [[ else -]]
      secretName: [[ printf "istio.%s" .Spec.ServiceAccountName ]]
      [[ end -]]
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ kubectl get mutatingwebhookconfiguration istio-sidecar-injector -o yaml
apiVersion: admissionregistration.k8s.io/v1beta1
kind: MutatingWebhookConfiguration
metadata:
  creationTimestamp: "2019-03-13T05:09:41Z"
  generation: 2
  labels:
    app: istio-sidecar-injector
    chart: sidecarInjectorWebhook-1.0.6
    heritage: Tiller
    release: istio
  name: istio-sidecar-injector
  resourceVersion: "1079"
  selfLink: /apis/admissionregistration.k8s.io/v1beta1/mutatingwebhookconfigurations/istio-sidecar-injector
  uid: 357f78e8-454e-11e9-9172-06ad1f6e4ae4
webhooks:
- clientConfig:
    caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1akNDQWM2Z0F3SUJBZ0lSQUlpYWdNOXVzU2pKVklvcU10dHAzRVV3RFFZSktvWklodmNOQVFFTEJRQXcKSERFYU1CZ0dBMVVFQ2hNUmF6aHpMbU5zZFhOMFpYSXViRzlqWVd3d0hoY05NVGt3TXpFek1EVXdPVFV6V2hjTgpNakF3TXpFeU1EVXdPVFV6V2pBY01Sb3dHQVlEVlFRS0V4RnJPSE11WTJ4MWMzUmxjaTVzYjJOaGJEQ0NBU0l3CkRRWUpLb1pJaHZjTkFRRUJCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFMR0htdGlzTFR4S3VXTTNZVlorQk1GUlNMZnYKU1E3QzlxdDNBc21KZW84clZBV1l0emsrYm42MTMwUnlid25kSWlqV2hlTC9qMTY3eWpZdHRFNzZUdnM4K1M3SwozYlNRNUFsZy9rOWNrbm4reTRsSFZUcWk1dWdZZHdLT29QYUdXZDVadlBnUDB2T3prdEc3aU9WTjZSZE80QjhIClY3ak8veUc1SnFzQVVpVmhtS3BpanlCN04xdmhDbkZjY0gzYnh2ZXdhSnlvRk56U3hycStJREhnMWI0OE9VTkkKVmlPczlzYUNCQndBVDRzWTRnZlRyYlErckFVTVZ6N1lRVWExSVM2RVhhelBwK3BtNW9hQUF6cGlnQkZmM1dLMwpVOE5MY2lIY3VoQWV4RnpTVkdFWEdDVldjcDZaOXQ0VWRHL2E3ZjdqazhNN3NZM1ZZNGdnRUtVbnk5RUNBd0VBCkFhTWpNQ0V3RGdZRFZSMFBBUUgvQkFRREFnSUVNQThHQTFVZEV3RUIvd1FGTUFNQkFmOHdEUVlKS29aSWh2Y04KQVFFTEJRQURnZ0VCQUtXem9OMjMyMWhUMm4ySDN2cjRTTDVadndBM0hXdytxSVVqT0F0M3NSaE1kUDJjdnRLVApsSTRja0trczhCcXo5V2I0aTdvYVlQeXFjcUtPUmxqZVR0ZnRNZWk0NDhLZnZWVWN2TWtSdkFjNllxNlZ0cG1hCitDZm0rR2R5UnlNM3lCY1NJZHhxbGF4cDliR1BaOVlFSmpzdm1FTGR1N3JFd2lTN0cxcEh0bWwyeTFtblVJcWYKN0dSQzJXeFdnTTZMb2d6aFlrb0swUjFlbk5OSExHZi9yeTZoMmVOQUxzMlp2UEJWWDBwVGIxMElieGZIbVBuUQpEM3hROTkwMEFLWnZkbFJYUm96bWFVNEdoVUh6ZHpheStmVnRQaWZJNWhxR1hOT2VNY0d0NkUxbG9FWUcxZnBFCi9RSnJ2NThQZGlTOUtkRlZka3FMb1BYR0ZuMlg0ak5sOVpJPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    service:
      name: istio-sidecar-injector
      namespace: istio-system
      path: /inject
  failurePolicy: Fail
  name: sidecar-injector.istio.io
  namespaceSelector:
    matchLabels:
      istio-injection: enabled
  rules:
  - apiGroups:
    - ""
    apiVersions:
    - v1
    operations:
    - CREATE
    resources:
    - pods
niclas@niclas-vm:~/work/tmp1/tmp/tmp/ncc3$ 

